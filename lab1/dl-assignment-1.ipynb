{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First assignment for the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----\n",
    "\n",
    "**Names: Luka Mucko, Luca Poli**\n",
    "\n",
    "**Group: 46**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Experiment with gradient descent optimization;\n",
    "2. Derive and implement gradients for binary cross-entropy loss, the sigmoid function and a linear layer;\n",
    "3. Test your gradient implementations with the finite difference method;\n",
    "4. Use these components to implement and train a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=6, linewidth=200)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1 Gradient descent optimization (6 points)\n",
    "\n",
    "Consider the following function with two parameters and its derivatives:\n",
    "\\begin{align}\n",
    "  f(x, y) &= x^2 + y^2 + x (y + 2) + \\cos(3x) \\\\\n",
    "  \\frac{\\partial f}{\\partial x} &= 2x - 3\\sin(3x) + y + 2 \\\\\n",
    "  \\frac{\\partial f}{\\partial y} &= x + 2y \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x ** 2 + y ** 2 + x * (y + 2) + np.cos(3 * x)\n",
    "def grad_x_f(x, y):\n",
    "    return 2 * x - 3 * np.sin(3 * x) + y + 2\n",
    "def grad_y_f(x, y):\n",
    "    return x + 2 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A plot of the function shows that it has multiple local minima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_f_contours():\n",
    "    xx, yy = np.meshgrid(np.linspace(-5, 5), np.linspace(-5, 5))\n",
    "    zz = f(xx, yy)\n",
    "    plt.contourf(xx, yy, zz, 50)\n",
    "    plt.contour(xx, yy, zz, 50, alpha=0.2, colors='black', linestyles='solid')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plot_f_contours()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implement gradient descent\n",
    "\n",
    "We would like to find the minimum of this function using gradient descent.\n",
    "\n",
    "**(a) Implement the gradient descent updates for $x$ and $y$ in the function below:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_f(x, y, step_size, steps):\n",
    "    # keep track of the parameters we tried so far\n",
    "    x_hist, y_hist = [x], [y]\n",
    "\n",
    "    # run gradient descent for the number of steps\n",
    "    for step in range(steps):\n",
    "        # compute the gradients at the current point\n",
    "        dx = grad_x_f(x, y)\n",
    "        dy = grad_y_f(x, y)\n",
    "\n",
    "        # apply the gradient descent updates to x and y\n",
    "        x = x - step_size*dx # TODO: compute the update\n",
    "        y = y - step_size*dy # TODO: compute the update\n",
    "\n",
    "        # store the new parameters\n",
    "        x_hist.append(x)\n",
    "        y_hist.append(y)\n",
    "\n",
    "    return x, y, f(x, y), x_hist, y_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following assert statements check that your implementation behaves sensibly\n",
    "# Use it to get a hint only if you are stuck.\n",
    "assert optimize_f(3, 2, 0.1, 1)[0] != 3, \"Hint: you are not changing `x`\"\n",
    "assert optimize_f(3, 2, 0.1, 1)[2] < f(3, 2), \"Hint: the function value is increasing, you should be minimizing it, not maximizing.\"\n",
    "assert abs(optimize_f(3, 2, 0.1, 1)[0] - 3) < 1, \"Hint: you are probably taking steps that are too large.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Tune the parameters\n",
    "\n",
    "We will now try if our optimization method works.\n",
    "\n",
    "Use this helper function to plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function that plots the results of the gradient descent optimization\n",
    "def plot_gradient_descent_results(x, y, val, x_hist, y_hist):\n",
    "    # plot the path on the contour plot\n",
    "    plt.figure(figsize=(20, 7))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_f_contours()\n",
    "    plt.plot(x_hist, y_hist, '.-')\n",
    "    \n",
    "    # plot the learning curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(f(np.array(x_hist), np.array(y_hist)), '.r-')\n",
    "    plt.title('Minimum value: %f' % f(x_hist[-1], y_hist[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(b) Run the gradient descent optimization with the following initial settings:**\n",
    "\n",
    "``x=3, y=2, step_size=0.1, steps=10``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = optimize_f(x=3, y=2, step_size=0.1, steps=20)\n",
    "plot_gradient_descent_results(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(c) Does it find the minimum of the function? What happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The algorithm got stuck in the local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(d) Try a few different values for the `step_size` and the number of `steps` to get close to the optimal solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: tune the parameters to find a better optimum\n",
    "results = optimize_f(x=3, y=2, step_size=0.315, steps=6)\n",
    "plot_gradient_descent_results(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(e) What happens if you set the step size too small? And what if it is too large?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If the step_size is too small the algorithm will most likely get stuck in a local minimum (given large steps, otherwise it will slowly go in the direction if $-\\nabla f(x,y)$) because the new point is too close to the previous (e.g. step_size=0.1, size=10 in the previous example). \\\n",
    "If the step_size is too large the algorithm will \"shoot off\" to a point that could be higher and then oscilate around the minimum, local or global (e.g. $f(x)=x^2$ and $\\eta=1$ for x=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(f) Were you able to find a step size that reached the global optimum? If not, why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "step_size=0.315 and steps=6 got pretty close to the global optima but not exactly. There is no guarrantee that with a fixed step_size we could reach the global (or local) optimum exactly. If we are in the vicinity of a global optimum (point $x$) the algorithm will reach the minimum if we set the new step_size as: $\\eta = \\arg \\min_{\\eta} (x - \\eta \\nabla f(x))$ but this is generally hard or impossible to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Implement a decreasing step size\n",
    "\n",
    "You might get better results if you use a step size that is large at the beginning, but slowly decreases during the optimization.\n",
    "\n",
    "Try the following scheme to compute the step size $\\eta_t$ in step $t$, given a decay parameter $d$:\n",
    "\\begin{align}\n",
    "  \\eta_t = \\eta_0 d^t\n",
    "\\end{align}\n",
    "\n",
    "**(g) Update your optimization function to use this step size schedule:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_f(x, y, step_size, steps, decay=1.0):\n",
    "    # keep track of the parameters we tried so far\n",
    "    x_hist, y_hist = [x], [y]\n",
    "\n",
    "    # run gradient descent for the number of steps\n",
    "    for step in range(steps):\n",
    "        # compute the gradients at this point\n",
    "        dx = grad_x_f(x, y)\n",
    "        dy = grad_y_f(x, y)\n",
    "\n",
    "        eta=step_size * np.power(decay, step)\n",
    "        # apply the gradient descent updates to x and y\n",
    "        x = x  - eta*dx# TODO: compute the update including step size decay\n",
    "        y = y  - eta*dy# TODO: compute the update including step size decay\n",
    "\n",
    "        # store the new parameters\n",
    "        x_hist.append(x)\n",
    "        y_hist.append(y)\n",
    "\n",
    "    return x, y, f(x, y), x_hist, y_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following assert statement checks that your implementation behaves sensibly\n",
    "_trace = optimize_f(0.123, 0.456, 0.01, 2, 0.1)[3]\n",
    "assert abs(_trace[1] - _trace[0]) > 5 * abs(_trace[2] - _trace[1]), \"Hint: step size should be decreasing\"\n",
    "del _trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(h) Tune the `step_sizes`, `steps` and `decay` parameters to get closer to the global minimum:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: tune the parameters to find the global optimum\n",
    "results = optimize_f(x=3, y=2, step_size=0.2, steps=10, decay=0.95)\n",
    "plot_gradient_descent_results(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert results[2] < -2, \"Hint: get closer to the optimum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will now look at some more complex functions that we can try to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.2 Neural network components (16 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this assignment, we will implement a simple neural network from scratch. We need four components:\n",
    "1. A sigmoid activation function,\n",
    "2. A ReLU activation function,\n",
    "3. A binary cross-entropy loss function,\n",
    "4. A linear layer.\n",
    "\n",
    "For each component, we will implement the forward pass, the backward pass, and the gradient descent update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Sigmoid non-linearity\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "![Sigmoid](attachment:sigmoid.png)\n",
    "\n",
    "**(a) Give the derivative of the sigmoid function:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\sigma(x)}{\\partial x} &= \\frac{e^{-x}}{(1 + e^{-x})^2} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(b) Implement the sigmoid and its gradient in the functions `sigmoid(x)` and `sigmoid_grad(x)`:<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "\n",
    "# try with a random input\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.uniform(-10, 10, size=5)\n",
    "print('x:', x)\n",
    "print('sigmoid(x):', sigmoid(x))\n",
    "print('sigmoid_grad(x):', sigmoid_grad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To check that the gradient implementation is correct, we can compute the numerical derivative using the [finite difference](https://en.wikipedia.org/wiki/Finite_difference) method. From [Chapter 11.5 of the Deep Learning book](http://www.deeplearningbook.org/contents/guidelines.html):\n",
    "\n",
    "> Because\n",
    "  \\begin{align}\n",
    "    f'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(x + \\epsilon) - f(x)}{ \\epsilon},\n",
    "  \\end{align}\n",
    "  we can approximate the derivative by using a small, finite $\\epsilon$:\n",
    "  \\begin{align}\n",
    "    f'(x) \\approx \\frac{f(x + \\epsilon) - f(x)}{\\epsilon}.\n",
    "  \\end{align}\n",
    "  We can improve the accuracy of the approximation by using the centered difference:\n",
    "  \\begin{align}\n",
    "    f'(x) \\approx \\frac{f(x + \\frac{1}{2} \\epsilon) - f(x - \\frac{1}{2} \\epsilon)}{\\epsilon}.\n",
    "  \\end{align}\n",
    "  The perturbation size $\\epsilon$ must be large enough to ensure that the perturbation is not rounded down too much by ﬁnite-precision numerical computations.\n",
    "\n",
    "**(c) Use the central difference method to check your implementation of the sigmoid gradient. Compute the numerical gradient and check that it is close to the symbolic gradient computed by your implementation:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start with some random inputs\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.uniform(-2, 2, size=5)\n",
    "\n",
    "# compute the symbolic gradient\n",
    "print('Symbolic ', sigmoid_grad(x))\n",
    "\n",
    "# TODO: compute the numerical gradient\n",
    "\n",
    "h = 0.001\n",
    "sigmoid_grad_cdm = (sigmoid(x + h/2) - sigmoid(x - h/2)) / h\n",
    "print('Numerical', sigmoid_grad_cdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(d) Is the gradient computed with finite differences exactly the same as the analytic answer? Why (not)?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With the finite differences we get the same result up to a certain precision. This is because the finite differences are an approximation of the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**If there is a big difference between the two gradients, please try to make this as small as possible before you continue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Rectified linear units (ReLU)\n",
    "\n",
    "The rectified linear unit is defined as:\n",
    "\\begin{align}\n",
    "  f(x) = \\max(0, x)\n",
    "\\end{align}\n",
    "\n",
    "![relu.png](attachment:relu.png)\n",
    "\n",
    "**(e) Give the derivative of the ReLU function:<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: this gradient is not well-defined everywhere, but make a sensible choice for all values of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\\begin{align}\n",
    "  \\frac{\\partial f(x)}{\\partial x} &= \\text{if } x < 0: 0 \\text{ else } 1 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(f) Implement the ReLU function and its gradient in the functions `relu(x)` and `relu_grad(x)`. Use the finite difference method to check that the gradient is correct:<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "# try with a random input\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.uniform(-10, 10, size=5)\n",
    "print('x:', x)\n",
    "print('relu(x):', relu(x))\n",
    "print('relu_grad(x):', relu_grad(x))\n",
    "\n",
    "\n",
    "h = 0.001\n",
    "relu_grad_cdm = (relu(x + h/2) - relu(x - h/2)) / h\n",
    "print('Numerical relu_grad(x):', relu_grad_cdm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Comparing sigmoid and ReLU\n",
    "\n",
    "The sigmoid and ReLU activation functions have slightly different characteristics.\n",
    "\n",
    "**(g) Run the code below to plot the sigmoid and ReLU activation functions and their gradients:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, sigmoid(x), label='Sigmoid')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, relu(x), label='ReLU')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, sigmoid_grad(x), label='Sigmoid gradient')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, relu_grad(x), label='ReLU gradient')\n",
    "plt.xlabel('x')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(h) Which activation function would you recommend for a network that outputs probabilities, i.e., outputs $\\in [0, 1]$? Why?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We would recommend the sigmoid function for probabilities, because it is bounded between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(i) Compare the gradients for sigmoid and ReLU. What are the advantages and disadvantages of each activation function in terms of their gradient?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The ReLu gradient is either 0 or 1, which makes it easier to compute. The sigmoid gradient is more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Binary cross-entropy loss\n",
    "\n",
    "We will use the binary cross-entropy loss to train our network. This loss function is useful for binary classification.\n",
    "\n",
    "The binary cross-entropy (BCE) is a function of the ground truth label $y \\in \\{0, 1\\}$ and the predicted label $\\hat{y} \\in [0, 1]$:\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathcal{L} &= -(y \\log{\\hat{y}} + (1-y) \\log(1-\\hat{y})) \\\\\n",
    "\\end{align}\n",
    "\n",
    "To minimize the BCE loss with gradient descent, we need to compute the gradient with respect to the prediction $\\hat{y}$.\n",
    "\n",
    "**(j) Derive the gradient for the BCE loss:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} &= -(\\frac{y}{\\hat{y}}-\\frac{1-y}{1-\\hat{y}})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**(k) Implement `bce_loss(y, y_hat)` and `bce_loss_grad(y, y_hat)` and use the finite difference method to check that the gradient is correct:<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bce_loss(y, y_hat):\n",
    "    # TODO: implement the BCE loss\n",
    "    return -(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))\n",
    "    raise NotImplementedError\n",
    "\n",
    "def bce_loss_grad(y, y_hat):\n",
    "    # TODO: implement the gradient of the BCE loss\n",
    "    return -(y/y_hat - (1-y)/(1-y_hat))\n",
    "    raise NotImplementedError\n",
    "\n",
    "# try with some random inputs\n",
    "rng = np.random.default_rng(12345)\n",
    "y = rng.integers(2, size=5)\n",
    "y_hat = rng.uniform(0, 1, size=5)\n",
    "print('y:', y)\n",
    "print('y_hat:', y_hat)\n",
    "print('bceloss(y, y_hat):', bce_loss(y, y_hat))\n",
    "print()\n",
    "\n",
    "def bce_loss_grad_numerical(y, y_hat):\n",
    "    h=0.001\n",
    "    return (bce_loss(y,y_hat+h) - bce_loss(y,y_hat-h))/(2*h)\n",
    "\n",
    "print(\"bce_loss_grad\", bce_loss_grad(y,y_hat))\n",
    "print(\"bce_loss_grad_numerical\", bce_loss_grad_numerical(y,y_hat))\n",
    "print(\"difference:\", np.abs(bce_loss_grad(y,y_hat) - bce_loss_grad_numerical(y,y_hat)))\n",
    "# TODO: compute and compare the symbolic and numerical gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Linear layer\n",
    "\n",
    "Finally, we need to compute the gradients for the linear layer in our network.\n",
    "\n",
    "Define a linear model $\\mathbf{y} = \\mathbf{x} \\mathbf{W} + \\mathbf{b}$, where\n",
    "* $\\mathbf{x}$ is an input vector of shape $N$,\n",
    "* $\\mathbf{W}$ is a weight matrix of shape $N \\times M$,\n",
    "* $\\mathbf{b}$ is a bias vector of shape $M$,\n",
    "* $\\mathbf{y}$ is the output vector of shape $M$.\n",
    "\n",
    "**(l) Derive the gradients for $\\mathbf{y}$ with respect to the input $\\mathbf{x}$ and the parameters $\\mathbf{W}$ and $\\mathbf{b}$:<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "_Hint: If you have trouble computing this in matrix notation directly, try to do the computation with scalars, writing the linear model as_\n",
    "\\begin{align}\n",
    "  y_j = \\sum_{i=1}^{N} x_i W_{ij} + b_j\n",
    "\\end{align}\n",
    "where $j$ ranges from $1$ to $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "TODO: Your answer here.\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial y_j}{\\partial x_i} &= W_{ij} &\n",
    "  \\frac{\\partial y_j}{\\partial W_{ik}} &= \\delta_{jk}x_i &\n",
    "  \\frac{\\partial y_j}{\\partial b_k} &=  \\delta_{jk} \\\\\n",
    "\\end{align}\n",
    "or\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} &= TODO &\n",
    "  \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} &= TODO &\n",
    "  \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{b}} &= TODO \\\\\n",
    "\\end{align}\n",
    "(keep only one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(m) Given the gradient $\\nabla_\\mathbf{y} \\mathcal{L}$ for the loss w.r.t. $\\mathbf{y}$, use the chain rule to derive the gradients for the loss w.r.t. $\\mathbf{x}$, $\\mathbf{W}$ and $\\mathbf{b}$:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here.\n",
    "\n",
    "\\begin{align}\n",
    "  \\nabla_\\mathbf{x} \\mathcal{L} &= \\nabla_\\mathbf{y} \\mathcal{L} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\nabla_\\mathbf{y} \\mathcal{L} W^T \\\\\n",
    "  \\nabla_\\mathbf{W} \\mathcal{L} &= \\nabla_\\mathbf{y} \\mathcal{L} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} = x^T \\nabla_\\mathbf{y} \\mathcal{L} \\\\\n",
    "  \\nabla_\\mathbf{b} \\mathcal{L} &= \\nabla_\\mathbf{y} \\mathcal{L} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{b}} = \\mathbf{1}^T \\nabla_\\mathbf{y} \\mathcal{L} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Implement a one-layer model (2 points)\n",
    "\n",
    "We can now implement a simple one-layer model with a sigmoid activation:\n",
    "\n",
    "1. Given an input vector $\\mathbf{x}$, weight vector $\\mathbf{w}$ and bias $b$, compute the output $\\hat{y}$:\n",
    "\n",
    "\\begin{align}\n",
    "h = \\mathbf{x} \\mathbf{w}^T + b \\\\\n",
    "\\hat{y} = \\sigma(h) \\\\\n",
    "\\end{align}\n",
    "\n",
    "2. Compute the BCE loss comparing the prediction $\\hat{y}$ with the ground-truth label $y$.\n",
    "\n",
    "3. Compute the gradient for the BCE loss and back-propagate this to get $\\nabla_\\mathbf{x} \\mathcal{L}$, the gradient of $\\mathcal{L}$ w.r.t. $\\mathbf{x}$.\n",
    "\n",
    "Hint: in numpy inner product and matrix multiplication is denoted as `np.dot(A, B)` or as `A @ B`.\n",
    "\n",
    "**(a) Complete the implementation below:<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 2.309880244091049\n",
      "Gradient [ 1.282477 -1.138274  0.784228  0.233444  0.067864]\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "rng = np.random.default_rng(12345)\n",
    "w = rng.normal(size=5)\n",
    "b = rng.normal()\n",
    "\n",
    "# implement the model\n",
    "def fn(x, y):\n",
    "    # TODO: forward: compute h, y_hat, loss\n",
    "    h = x @ w.T + b\n",
    "    y_hat = sigmoid(h)\n",
    "    loss = bce_loss(y, y_hat)\n",
    "    \n",
    "    # TODO: backward: compute grad_y_hat, grad_h, grad_x\n",
    "    grad_y_hat = bce_loss_grad(y, y_hat)\n",
    "    grad_h = grad_y_hat * sigmoid_grad(h)\n",
    "    grad_x = grad_h * w.T\n",
    "    \n",
    "    return loss, grad_x\n",
    "\n",
    "# test with a random input\n",
    "x = rng.uniform(size=5)\n",
    "y = 1\n",
    "\n",
    "loss, grad_x = fn(x, y)\n",
    "print(\"Loss\", loss)\n",
    "print(\"Gradient\", grad_x)\n",
    "\n",
    "assert np.isscalar(loss), \"Loss should be scalar\"\n",
    "assert grad_x.shape == x.shape, \"Gradient should have same shape as x\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Use the finite-difference method to check the gradient $\\nabla_\\mathbf{x} \\mathcal{L}$:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic gradient\n",
      "[ 1.177245 -1.044874  0.719879  0.214289  0.062295]\n",
      "Numerical gradient\n",
      "[ 1.177245 -1.044874  0.719879  0.214289  0.062295]\n"
     ]
    }
   ],
   "source": [
    "# start with some random inputs\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.uniform(size=5)\n",
    "y = 1\n",
    "\n",
    "# set epsilon to a small value\n",
    "eps = 0.00001\n",
    "\n",
    "numerical_grad = np.zeros(x.shape)\n",
    "# compute the gradient for each element of x separately\n",
    "for i in range(len(x)):\n",
    "    # compute inputs at -eps/2 and +eps/2\n",
    "    x_a, x_b = x.copy(), x.copy()\n",
    "    x_a[i] += eps / 2\n",
    "    x_b[i] -= eps / 2\n",
    "\n",
    "    # compute the gradient for this element\n",
    "    loss_a, _ = fn(x_a, y)\n",
    "    loss_b, _ = fn(x_b, y)\n",
    "    numerical_grad[i] = (loss_a - loss_b) / eps\n",
    "\n",
    "# compute the symbolic gradient\n",
    "loss, symbolic_grad = fn(x, y)\n",
    "    \n",
    "print(\"Symbolic gradient\")\n",
    "print(symbolic_grad)\n",
    "print(\"Numerical gradient\")\n",
    "print(numerical_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Implement a linear layer and the sigmoid and ReLU activation functions (5 points)\n",
    "\n",
    "We will now construct a simple neural network. We need to implement the following objects:\n",
    "* `Linear`: a layer that computes `y = x*W + b`.\n",
    "* `Sigmoid`: a layer that computes `y = sigmoid(x)`.\n",
    "* `ReLU`: a layer that computes `y = relu(x)`.\n",
    "\n",
    "For each layer class, we need to implement the following methods:\n",
    "* `forward`: The forward pass that computes the output `y` given `x`.\n",
    "* `backward`: The backward pass that receives the gradient for `y` and computes the gradients for the input `x` and the parameters of the layer.\n",
    "* `step`: The update step that applies the gradient updates to the parameters of the layer, based on the gradient computed and stored by `backward`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Implement a class `Linear` that computes `y = x*W + b`:<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes y = x * w + b.\n",
    "class Linear:\n",
    "    def __init__(self, n_in, n_out, rng = np.random.default_rng(12345)):\n",
    "        # initialize the weights randomly,\n",
    "        # using the Xavier initialization rule for scale\n",
    "        a = np.sqrt(6 / (n_in * n_out))\n",
    "        self.W = rng.uniform(-a, a, size=(n_in, n_out))\n",
    "        self.b = np.zeros((n_out,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: compute the forward pass\n",
    "        y = 0  # TODO\n",
    "        return y\n",
    "\n",
    "    def backward(self, x, dy):\n",
    "        # TODO: compute the backward pass,\n",
    "        # given dy, compute the gradients for x, W and b\n",
    "        dx = 0       # TODO\n",
    "        self.dW = 0  # TODO\n",
    "        self.db = 0  # TODO\n",
    "        return dx\n",
    "    \n",
    "    def step(self, step_size):\n",
    "        # TODO: apply a gradient descent update step\n",
    "        self.W = self.W  # TODO\n",
    "        self.b = self.b  # TODO\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Linear %dx%d' % self.W.shape\n",
    "\n",
    "\n",
    "# Try the new class with some random values.\n",
    "# Debugging tip: always choose a unique length for each dimension,\n",
    "#  so you'll get an error if you mix them up.\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.uniform(size=(3, 5))\n",
    "\n",
    "layer = Linear(5, 7, rng=rng)\n",
    "y = layer.forward(x)\n",
    "dx = layer.backward(x, np.ones_like(y))\n",
    "print('y:', y)\n",
    "print('dx:', dx)\n",
    "\n",
    "# Verify correctness\n",
    "assert y.shape == (3,7)\n",
    "assert dx.shape == x.shape\n",
    "layer.W *= 2\n",
    "layer.b = layer.b * 2 + 1\n",
    "y2 = layer.forward(x)\n",
    "dx2 = layer.backward(x, np.ones_like(y))\n",
    "assert np.all(y2 == 2 * y + 1)\n",
    "assert np.all(dx2 == 2 * dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Implement a class `Sigmoid` that computes `y = 1 / (1 + exp(-x))`:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes y = 1 / (1 + exp(-x)).\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        # TODO: compute the forward pass\n",
    "        raise NotImplementedError  # TODO\n",
    "\n",
    "    def backward(self, x, dy):\n",
    "        # TODO: compute the backward pass,\n",
    "        # return the gradient for x given the gradient for y\n",
    "        raise NotImplementedError  # TODO\n",
    "    \n",
    "    def step(self, step_size):\n",
    "        raise NotImplementedError  # TODO\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Sigmoid'\n",
    "\n",
    "\n",
    "# try the new class with some random values\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.normal(size=(3, 5))\n",
    "\n",
    "layer = Sigmoid()\n",
    "y = layer.forward(x)\n",
    "dx = layer.backward(x, np.ones_like(y))\n",
    "print('y:', y)\n",
    "print('dx:', dx)\n",
    "\n",
    "assert y.shape == x.shape, \"Output sigmoid should have the same shape as input\"\n",
    "assert dx.shape == x.shape, \"Gradient sigmoid should have the same shape as input\"\n",
    "assert np.all(y > 0) and np.all(y < 1), \"Output of sigmoid should be between 0 and 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Implement a class `ReLU` that computes `y = max(0, x)`:<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes y = max(0, x).\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        # TODO: compute the forward pass\n",
    "        raise NotImplementedError  # TODO\n",
    "\n",
    "    def backward(self, x, dy):\n",
    "        # TODO: compute the backward pass,\n",
    "        # return the gradient for x given dy\n",
    "        raise NotImplementedError  # TODO\n",
    "    \n",
    "    def step(self, step_size):\n",
    "        raise NotImplementedError  # TODO\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'ReLU'\n",
    "\n",
    "\n",
    "# try the new class with some random values\n",
    "rng = np.random.default_rng(12345)\n",
    "x = rng.uniform(-10, 10, size=(3, 5))\n",
    "\n",
    "layer = ReLU()\n",
    "y = layer.forward(x)\n",
    "dx = layer.backward(x, np.ones_like(y))\n",
    "print('y:', y)\n",
    "print('dx:', dx)\n",
    "\n",
    "assert y.shape == x.shape, \"Output of ReLU should have the same shape as input\"\n",
    "assert dx.shape == x.shape, \"Gradient of ReLU should have the same shape as input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the gradients\n",
    "\n",
    "The code below will check your implementations using SciPy's finite difference implementation [`check_grad`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.check_grad.html). This is similar to what we did manually before, but automates some of the work.\n",
    "\n",
    "**(d) Run the code and check that the error is not too large.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify gradient computations for Linear\n",
    "# test for dx\n",
    "rng = np.random.default_rng(12345)\n",
    "layer = Linear(5, 7, rng)\n",
    "def test_fn(x):\n",
    "    x = x.reshape(3, 5)\n",
    "    # multiply the output with a constant to check if\n",
    "    # the gradient uses dy\n",
    "    return 2 * np.sum(layer.forward(x))\n",
    "def test_fn_grad(x):\n",
    "    x = x.reshape(3, 5)\n",
    "    # multiply the incoming dy gradient with a constant\n",
    "    return layer.backward(x, 2 * np.ones((3, 7))).flatten()\n",
    "\n",
    "err = scipy.optimize.check_grad(test_fn, test_fn_grad, rng.uniform(-10, 10, size=3 * 5))\n",
    "print(\"err on dx:\", err)\n",
    "assert np.abs(err) < 1e-5, \"Error on dx is too large, check your implementation of Linear.backward.\"\n",
    "\n",
    "# test for dW\n",
    "x = rng.uniform(size=(3, 5))\n",
    "layer = Linear(5, 7, rng)\n",
    "def test_fn(w):\n",
    "    layer.W = w.reshape(5, 7)\n",
    "    # multiply the output with a constant to check if\n",
    "    # the gradient uses dy\n",
    "    return 2 * np.sum(layer.forward(x))\n",
    "def test_fn_grad(w):\n",
    "    layer.W = w.reshape(5, 7)\n",
    "    # multiply the incoming dy gradient with a constant\n",
    "    layer.backward(x, 2 * np.ones((3, 7)))\n",
    "    return layer.dW.flatten()\n",
    "\n",
    "err = scipy.optimize.check_grad(test_fn, test_fn_grad, rng.uniform(-10, 10, size=5 * 7))\n",
    "print(\"err on dW:\", err)\n",
    "assert np.abs(err) < 1e-5, \"Error on dW is too large, check your implementation of Linear.backward.\"\n",
    "\n",
    "# test for db\n",
    "x = rng.uniform(size=(3, 5,))\n",
    "layer = Linear(5, 7, rng)\n",
    "def test_fn(b):\n",
    "    layer.b = b\n",
    "    # multiply the output with a constant to check if\n",
    "    # the gradient uses dy\n",
    "    return 2 * np.sum(layer.forward(x))\n",
    "def test_fn_grad(b):\n",
    "    layer.b = b\n",
    "    # multiply the incoming dy gradient with a constant\n",
    "    layer.backward(x, 2 * np.ones((x.shape[0], 7)))\n",
    "    return layer.db\n",
    "\n",
    "err = scipy.optimize.check_grad(test_fn, test_fn_grad, rng.uniform(-10, 10, size=7))\n",
    "print(\"err on db:\", err)\n",
    "assert np.abs(err) < 1e-5, \"Error on db is too large, check your implementation of Linear.backward.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify gradient computation for Sigmoid\n",
    "# test for dx\n",
    "layer = Sigmoid()\n",
    "def test_fn(x):\n",
    "    # multiply the output with a constant to check if\n",
    "    # the gradient uses dy\n",
    "    return np.sum(2 * layer.forward(x))\n",
    "def test_fn_grad(x):\n",
    "    # multiply the incoming dy gradient with a constant\n",
    "    return layer.backward(x, 2 * np.ones(x.shape))\n",
    "\n",
    "rng = np.random.default_rng(12345)\n",
    "err = scipy.optimize.check_grad(test_fn, test_fn_grad, rng.uniform(-10, 10, size=5))\n",
    "print(\"err on dx:\", err)\n",
    "assert np.abs(err) < 1e-5, \"Error on dx is too large, check your implementation of Sigmoid.backward.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify gradient computation for ReLU\n",
    "# test for dx\n",
    "layer = ReLU()\n",
    "def test_fn(x):\n",
    "    # multiply the output with a constant to check if\n",
    "    # the gradient uses dy\n",
    "    return 2 * np.sum(layer.forward(x))\n",
    "def test_fn_grad(x):\n",
    "    # multiply the incoming dy gradient with a constant\n",
    "    return layer.backward(x, 2 * np.ones(x.shape))\n",
    "\n",
    "rng = np.random.default_rng(12345)\n",
    "err = scipy.optimize.check_grad(test_fn, test_fn_grad, rng.uniform(1, 10, size=5))\n",
    "print(\"err on dx:\", err)\n",
    "assert np.abs(err) < 1e-5, \"Error on dx is too large, check your implementation of ReLU.backward.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Construct a neural network with back-propagation\n",
    "\n",
    "We will use the following container class to implement the network:\n",
    "1. The `forward` pass computes the output of each layer. We store the intermediate inputs for the backward pass.\n",
    "2. The `backward` pass computes the gradients for each layer, in reverse order, by using the original input `x` and the gradient `dy` from the previous layer.\n",
    "3. The `step` function will ask each layer to apply the gradient descent updates to its weights.\n",
    "\n",
    "**(a) Read the code below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute the forward pass for each layer\n",
    "        trace = []\n",
    "        for layer in self.layers:\n",
    "            # compute the forward pass\n",
    "            y = layer.forward(x)\n",
    "            # store the original input for the backward pass\n",
    "            trace.append((layer, x))\n",
    "            x = y\n",
    "        # return the final output and the history trace\n",
    "        return y, trace\n",
    "\n",
    "    def backward(self, trace, dy):\n",
    "        # compute the backward pass for each layer\n",
    "        for layer, x in trace[::-1]:\n",
    "            # compute the backward pass using the original input x\n",
    "            dy = layer.backward(x, dy)\n",
    "\n",
    "    def step(self, learning_rate):\n",
    "        # apply the gradient descent updates of each layer\n",
    "        for layer in self.layers:\n",
    "            layer.step(learning_rate)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(str(l) for l in self.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Training the network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a simple dataset with 360 handwritten digits.\n",
    "\n",
    "Each sample has $8 \\times 8$ pixels, arranged as a 1D vector of 64 features.\n",
    "\n",
    "We create a binary classification problem with the label 0 for the digits 0 to 4, and 1 for the digits 5 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first two classes of the digits dataset\n",
    "dataset = sklearn.datasets.load_digits()\n",
    "digits_x, digits_y = dataset['data'], dataset['target']\n",
    "\n",
    "# create a binary classification problem\n",
    "digits_y = (digits_y < 5).astype(float)\n",
    "\n",
    "# plot some of the digits\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(np.hstack([digits_x[i].reshape(8, 8) for i in range(10)]), cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.axis('off')\n",
    "\n",
    "# normalize the values to [0, 1]\n",
    "digits_x -= np.mean(digits_x)\n",
    "digits_x /= np.std(digits_x)\n",
    "\n",
    "# print some statistics\n",
    "print('digits_x.shape:', digits_x.shape)\n",
    "print('digits_y.shape:', digits_y.shape)\n",
    "print('min, max values:', np.min(digits_x), np.max(digits_x))\n",
    "print('labels:', np.unique(digits_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the dataset in a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 50%/50% train/test split\n",
    "train_prop = 0.5\n",
    "n_train = int(digits_x.shape[0] * train_prop)\n",
    "\n",
    "# shuffle the images\n",
    "rng = np.random.default_rng(12345)\n",
    "idxs = rng.permutation(digits_x.shape[0])\n",
    "\n",
    "# take a subset\n",
    "x = {'train': digits_x[idxs[:n_train]],\n",
    "     'test':  digits_x[idxs[n_train:]]}\n",
    "y = {'train': digits_y[idxs[:n_train]],\n",
    "     'test':  digits_y[idxs[n_train:]]}\n",
    "\n",
    "print('Training samples:', x['train'].shape[0])\n",
    "print('Test samples:', x['test'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a function that trains the network. For each epoch, it loops over all minibatches in the training set and updates the network weights. It will then compute the loss and accuracy for the test samples. Finally, it will plot the learning curves.\n",
    "\n",
    "**(a) Read through the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, x, y, epochs=25, learning_rate=0.001, mb_size=10):\n",
    "    # initialize the loss and accuracy history\n",
    "    loss_hist = {'train': [], 'test': []}\n",
    "    accuracy_hist = {'train': [], 'test': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # initialize the loss and accuracy for this epoch\n",
    "        loss = {'train': 0.0, 'test': 0.0}\n",
    "        accuracy = {'train': 0.0, 'test': 0.0}\n",
    "\n",
    "        # first train on training data, then evaluate on the test data\n",
    "        for phase in ('train', 'test'):\n",
    "            # compute the number of minibatches\n",
    "            steps = x[phase].shape[0] // mb_size\n",
    "\n",
    "            # loop over all minibatches\n",
    "            for step in range(steps):\n",
    "                # get the samples for the current minibatch\n",
    "                x_mb = x[phase][(step * mb_size):((step + 1) * mb_size)]\n",
    "                y_mb = y[phase][(step * mb_size):((step + 1) * mb_size), None]\n",
    "\n",
    "                # compute the forward pass through the network\n",
    "                pred_y, trace = net.forward(x_mb)\n",
    "\n",
    "                # compute the current loss and accuracy\n",
    "                loss[phase] += np.mean(bce_loss(y_mb, pred_y))\n",
    "                accuracy[phase] += np.mean((y_mb > 0.5) == (pred_y > 0.5))\n",
    "\n",
    "                # only update the network in the training phase\n",
    "                if phase == 'train':\n",
    "                    # compute the gradient for the loss\n",
    "                    dy = bce_loss_grad(y_mb, pred_y)\n",
    "\n",
    "                    # backpropagate the gradient through the network\n",
    "                    net.backward(trace, dy)\n",
    "\n",
    "                    # update the weights\n",
    "                    net.step(learning_rate)\n",
    "\n",
    "            # compute the mean loss and accuracy over all minibatches\n",
    "            loss[phase] = loss[phase] / steps\n",
    "            accuracy[phase] = accuracy[phase] / steps\n",
    "\n",
    "            # add statistics to history\n",
    "            loss_hist[phase].append(loss[phase])\n",
    "            accuracy_hist[phase].append(accuracy[phase])\n",
    "\n",
    "        print('Epoch %3d: loss[train]=%7.4f  accuracy[train]=%7.4f  loss[test]=%7.4f  accuracy[test]=%7.4f' %\n",
    "              (epoch, loss['train'], accuracy['train'], loss['test'], accuracy['test']))\n",
    "\n",
    "    # plot the learning curves\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for phase in loss_hist:\n",
    "        plt.plot(loss_hist[phase], label=phase)\n",
    "    plt.title('BCE loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for phase in accuracy_hist:\n",
    "        plt.plot(accuracy_hist[phase], label=phase)\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a two-layer network:\n",
    "* A linear layer that maps the 64 features of the input to 32 features.\n",
    "* A ReLU activation function.\n",
    "* A linear layer that maps the 32 features to the 1 output features.\n",
    "* A sigmoid activation function that maps the output to [0, 1].\n",
    "\n",
    "**(b) Train the network and inspect the results. Tune the hyperparameters to get a good result.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct network\n",
    "rng = np.random.default_rng(12345)\n",
    "net = Net([\n",
    "        Linear(64, 32, rng=rng),\n",
    "        ReLU(),\n",
    "        Linear(32, 1, rng=rng),\n",
    "        Sigmoid()])\n",
    "\n",
    "# TODO: tune the hyperparameters\n",
    "fit(net, x, y,\n",
    "    epochs = 25,\n",
    "    learning_rate = 0,\n",
    "    mb_size = 10)\n",
    "\n",
    "# Note: add more cells below if you want to keep runs with different hyperparameters. Make sure to also copy the network construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How did each of the hyperparameters (number of epochs, learning rate, minibatch size) influence your results? How important is it to set each correctly?<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Create and train a network with one linear layer followed by a sigmoid activation:<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "`net = Net([Linear(...), Sigmoid()]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Discuss your results. Compare the results of this single-layer network with those of the network you trained before.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Repeat the experiment with a network with two linear layers, followed by a sigmoid activation: `[Linear, Linear, Sigmoid]`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) How does the performance of this network compare with the previous networks. Can you explain this result? What is the influence of the activation functions in the network?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) One way to improve the performance of a neural network is by increasing the number of layers. Try a deeper network (e.g., a network with four linear layers) to see if this outperforms the previous networks.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) Discuss your findings. Were you able to obtain a perfect classification? Explain the learning curves.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Final questions (6 points)\n",
    "\n",
    "You now have some experience training neural networks. Time for a few final questions.\n",
    "\n",
    "**(a) What is the influence of the learning rate? What happens if the learning rate is too low or too high?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) What is the role of the minibatch size in SGD? Explain the downsides of a minibatch size that is too small or too high.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) In the linear layer, we initialized the weights $w$ with random values, but we initialized the bias $b$ with zeros. What would happen if the weights $w$ were initialised as zeros? Why is this not a problem for the bias?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 45 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 00f98aa / 2023-09-04</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
