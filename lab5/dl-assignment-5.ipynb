{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth assignment for the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names: Luka Mucko, Luca Poli**\n",
    "\n",
    "**Group: 46**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Construct a PyTorch `DataSet`\n",
    "1. Train and modify a transformer network\n",
    "1. Experiment with a translation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "If you haven't done so already, you will need to install the following additional libraries:\n",
    "* `torch` for PyTorch,\n",
    "* `d2l`, the library that comes with the [Dive into deep learning](https://d2l.ai) book.  \n",
    "  Note: if you get errors, make sure the right version of the d2l library is installed:\n",
    "  `pip install d2l==1.0.0a1.post0`\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.414614Z",
     "start_time": "2023-10-06T13:31:46.441214Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "from random import Random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (IterableDataset, DataLoader)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Learning to calculate (5 points)\n",
    "\n",
    "In this assignment we are going to train a neural network to do mathematics.\n",
    "When communicating between humans, mathematics is expressed with words and formulas.\n",
    "The simplest of these are formulas with a numeric answer. For example, we might ask what is `100+50`, to which the answer is `150`.\n",
    "\n",
    "To teach a computer how to do this task, we are going to need a dataset.\n",
    "\n",
    "Below is a function that generates a random formula. Study it, and see if you understand its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.499006Z",
     "start_time": "2023-10-06T13:31:59.412614Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_integer(length: int, signed: bool = True, rng: Random = Random()):\n",
    "    max = math.pow(10, length)\n",
    "    min = -max if signed else 0\n",
    "    return rng.randint(min, max)\n",
    "\n",
    "def random_formula(complexity: int, signed: bool = True, rng: Random = Random()):\n",
    "    \"\"\"\n",
    "    Generate a random formula of the form \"a+b\" or \"a-b\".\n",
    "    complexity is the maximum number of digits in the numbers.\n",
    "    \"\"\"\n",
    "    a = random_integer(complexity, signed, rng)\n",
    "    b = random_integer(complexity, False, rng)\n",
    "    is_addition = not signed or rng.choice([False, True])\n",
    "    if is_addition:\n",
    "        return (f\"{a}+{b}\", str(a + b))\n",
    "    else:\n",
    "        return (f\"{a}-{b}\", str(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.522343Z",
     "start_time": "2023-10-06T13:31:59.448043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('649+864', '1513')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123456\n",
    "random_formula(3, rng=Random(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rng` argument allows us to reproduce the same random numbers, which you can verify by running the code below multiple times. But if you change the seed to `None` then the random generator is initialized differently each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T14:09:52.891818Z",
     "start_time": "2023-10-06T14:09:52.864221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649+864 = 1513\n",
      "-940-819 = -1759\n",
      "954-2 = 952\n",
      "-896-274 = -1170\n",
      "-762-954 = -1716\n"
     ]
    }
   ],
   "source": [
    "def random_formulas(complexity, signed, count, seed):\n",
    "    \"\"\"\n",
    "    Iterator that yields the given count of random formulas\n",
    "    \"\"\"\n",
    "    rng = Random(seed)\n",
    "    for i in range(count):\n",
    "        yield random_formula(complexity, signed, rng=rng)\n",
    "\n",
    "for q, a in random_formulas(3, True, 5, seed):\n",
    "    print(f'{q} = {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat these expressions as sequences of tokens, where each character is a token. In addition we will need tokens to denote begin-of-sequence and end-of-sequence, as well as padding, for which we will use `'<bos>'`, `'<eos>'`, and `'<pad>'` respectively, as is done in the book.\n",
    "\n",
    "[d2l chapter 9.2](https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html) includes an example of tokenizing a string, and it also defines a `Vocab` class that handles converting the tokens to numbers.\n",
    "\n",
    "For this dataset we know beforehand what the vocabulary will be.\n",
    "\n",
    "### Creating a vocabulary\n",
    "\n",
    "**(a) What are the tokens in this dataset? Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.641823Z",
     "start_time": "2023-10-06T13:31:59.514094Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: fill in all possible tokens\n",
    "vocab = d2l.Vocab(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-'], reserved_tokens=['<bos>', '<eos>', '<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the vocabulary to double check that it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.689021Z",
     "start_time": "2023-10-06T13:31:59.550757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16\n",
      "Vocabulary: ['+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<bos>', '<eos>', '<pad>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', len(vocab))\n",
    "print('Vocabulary:', vocab.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the d2l Vocab class includes a `'<unk>'` token, for handling unknown tokens in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tokenize and encode formula.\n",
    "\n",
    "**(b) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.690022Z",
     "start_time": "2023-10-06T13:31:59.606646Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_encode(string: str, vocab=vocab) -> List[int]:\n",
    "    # TODO: Tokenize the string and encode using the vocabulary.\n",
    "    #       Include an end-of-string token (but not a begin-of-string token).\n",
    "    return vocab[list(string)] + [vocab['<eos>']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a random formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.909012Z",
     "start_time": "2023-10-06T13:31:59.636536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question 649+864 and answer 1513\n",
      "are encoded as [8, 6, 11, 0, 10, 8, 6, 13] and [3, 7, 3, 5, 13]\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "print('The question', q, 'and answer', a)\n",
    "print('are encoded as', tokenize_and_encode(q), 'and', tokenize_and_encode(a))\n",
    "\n",
    "# Check tokenize_and_encode\n",
    "assert ''.join(vocab.to_tokens(tokenize_and_encode(q))) == q + '<eos>'\n",
    "assert len(tokenize_and_encode(q)) == len(q) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and trimming\n",
    "\n",
    "Next, to be able to work with a whole dataset of these encoded sequences, they all need to be the same length.\n",
    "\n",
    "**(c) Implement the function below that pads or trims the encoded token sequence as needed.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: see [d2l section 10.5.3](http://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#loading-sequences-of-fixed-length) for a very similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.909012Z",
     "start_time": "2023-10-06T13:31:59.663556Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_or_trim(tokens: List[int], target_length: int, vocab=vocab):\n",
    "    if len(tokens) < target_length:\n",
    "        return tokens + [vocab['<pad>']] * (target_length - len(tokens))\n",
    "    elif len(tokens) > target_length:\n",
    "        return tokens[:target_length]\n",
    "    else:\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.911012Z",
     "start_time": "2023-10-06T13:31:59.685021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 6, 11, 0, 10, 8, 6, 13, 14, 14]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad or trim q to get a sequence of 10 tokens\n",
    "pad_or_trim(tokenize_and_encode(q), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.912013Z",
     "start_time": "2023-10-06T13:31:59.730345Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check pad_or_trim\n",
    "assert len(pad_or_trim([1,2,3,4,5],10)) == 10\n",
    "assert len(pad_or_trim(list(range(20)),10)) == 10\n",
    "assert vocab.to_tokens(pad_or_trim([1,2,3,4,5],10)[5:]) == ['<pad>','<pad>','<pad>','<pad>','<pad>'], \\\n",
    "       f\"Incorrect padding tokens, found {vocab.to_tokens(pad_or_trim([1,2,3,4,5],10)[5:])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating tokens\n",
    "\n",
    "We can use `vocab.to_tokens` to convert the encoded token sequence back to something more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.952866Z",
     "start_time": "2023-10-06T13:31:59.758201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '4', '9', '+', '8', '6', '4', '<eos>', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.to_tokens(pad_or_trim(tokenize_and_encode(q), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define the `decode_tokens` function to convert entire lists or tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:31:59.953864Z",
     "start_time": "2023-10-06T13:31:59.788099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5' '1' '3' '+' '1' '3' '2' '3' '<eos>' '<pad>']\n",
      " ['4' '1' '2' '+' '4' '2' '<eos>' '<pad>' '<pad>' '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "def decode_tokens(t, vocab=vocab):\n",
    "    # convert a list, tensor, or array of encoded tokens\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.detach().cpu().numpy()\n",
    "    t = np.asarray(t)\n",
    "    return np.asarray(vocab.to_tokens(list(t.flatten()))).reshape(*t.shape)\n",
    "\n",
    "# convert all tokens at once\n",
    "print(decode_tokens([pad_or_trim(tokenize_and_encode('513+1323'), 10),\n",
    "                     pad_or_trim(tokenize_and_encode('412+42'), 10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset\n",
    "\n",
    "The most convenient way to use a data generating function for training a neural network is to wrap it in a PyTorch `Dataset`. In this case, we will use an [IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset), which can be used as an iterator to walk over the samples in the dataset.\n",
    "\n",
    "**(d) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T14:13:06.891974Z",
     "start_time": "2023-10-06T14:13:06.858010Z"
    }
   },
   "outputs": [],
   "source": [
    "class FormulaDataset(IterableDataset):\n",
    "    def __init__(self, complexity, signed, count, seed=None, vocab=vocab):\n",
    "        self.seed = seed\n",
    "        self.complexity = complexity\n",
    "        self.signed = signed\n",
    "        self.count = count\n",
    "        self.vocab = vocab\n",
    "        self.max_question_length = 2 * complexity + 3\n",
    "        self.max_answer_length = complexity + 2\n",
    "\n",
    "        # TODO: Complete the class definition.\n",
    "        #       See the documentation for IterableDataset for examples.\n",
    "        #       Make sure that the values yielded by the iterator are pairs of torch tensors.\n",
    "        #       To create a repeatable dataset, always start with the same random seed.\n",
    "        data = random_formulas(complexity, signed, count, Random(seed))\n",
    "\n",
    "        self.data = [(torch.tensor(pad_or_trim(tokenize_and_encode(q), self.max_question_length, vocab)),\n",
    "                      torch.tensor(pad_or_trim(tokenize_and_encode(a), self.max_answer_length, vocab)))\n",
    "                     for q, a in data]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Define a training set with 10000 formulas and a validation set with 5000 formulas, both with complexity 3.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: make sure that the training and validation set are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T14:13:11.899800Z",
     "start_time": "2023-10-06T14:13:11.519809Z"
    }
   },
   "outputs": [],
   "source": [
    "complexity = 3\n",
    "signed = True\n",
    "# TODO: Your code here.\n",
    "train_data = FormulaDataset(complexity, signed, 10000, 123456)\n",
    "val_data   = FormulaDataset(complexity, signed, 5000, 1234789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we wrap each dataset in a `DataLoader` to create minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:33:31.090968Z",
     "start_time": "2023-10-06T13:33:31.039574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "batch_size = 125\n",
    "data_loaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=batch_size),\n",
    "    'val':   torch.utils.data.DataLoader(val_data, batch_size=batch_size),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:33:32.011430Z",
     "start_time": "2023-10-06T13:33:31.853622Z"
    }
   },
   "outputs": [],
   "source": [
    "# The code below checks that the datasets are defined correctly\n",
    "train_loader = data_loaders['train']\n",
    "val_loader = data_loaders['val']\n",
    "\n",
    "from typing import Tuple\n",
    "from typing_extensions import assert_type\n",
    "for (name, loader), expected_size in zip(data_loaders.items(), [10000,5000]):\n",
    "    first_batch = next(iter(loader))\n",
    "    assert len(first_batch) == 2, \\\n",
    "           f\"The {name} dataset should yield (question, answer) pairs when iterated over.\"\n",
    "    assert torch.is_tensor(first_batch[0]), \\\n",
    "           f\"The questions in the {name} dataset should be torch.tensors\"\n",
    "    assert tuple(first_batch[0].shape) == (batch_size, 2*complexity+3), \\\n",
    "           f\"The questions in the {name} dataset should be of size (batch_size, max_question_length), i.e. {batch_size,2*complexity+3}, found {tuple(first_batch[0].shape)}\"\n",
    "    assert first_batch[0].dtype in [torch.int32,torch.int64], \\\n",
    "           f\"The questions in the {name} dataset should be encoded as integers, found {first_batch[0].dtype}\"\n",
    "    assert torch.equal(next(iter(loader))[0], next(iter(loader))[0]), \\\n",
    "           f\"The {name} dataset should be deterministic, it should produce the same data each time\"\n",
    "    assert all([len(batch[0]) == batch_size for batch in iter(loader)]), \\\n",
    "           f\"Batches should all have the right size. Perhaps the batch size does not evenly divide the dataset size?\"\n",
    "    assert sum([len(batch[0]) for batch in iter(loader)]) == expected_size, \\\n",
    "           f\"{name} dataset does not have the right size, expected {expected_size}, found {sum([len(batch[0]) for batch in iter(loader)])}.\"\n",
    "assert not torch.equal(next(iter(train_loader))[0], next(iter(val_loader))[0]), \\\n",
    "       \"The training data and validation data should not be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Transformer inputs (10 points)\n",
    "\n",
    "There is a detailed description of the transformer model in [chapter 11 of the d2l book](http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html). We will not use most the code from the book, and instead use [PyTorch's built-in Transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n",
    "\n",
    "However, some details we still need to implement ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n",
    "\n",
    "Training a transformer uses masked self-attention, so we need some masks. Here are two functions that make these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(size, device=device):\n",
    "    \"\"\"\n",
    "    Mask that indicates that tokens at a position are not allowed to attend to\n",
    "    tokens in subsequent positions.\n",
    "    \"\"\"\n",
    "    mask = (torch.tril(torch.ones((size, size), device=device))) == 0\n",
    "    return mask\n",
    "\n",
    "def generate_padding_mask(tokens, padding_token):\n",
    "    \"\"\"\n",
    "    Mask that indicates which tokens should be ignored because they are padding.\n",
    "    \"\"\"\n",
    "    return tokens == torch.tensor(padding_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Generate a padding mask for a random encoded token string.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: make sure that `tokens` is a torch.tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8,  6, 11,  0, 10,  8,  6, 13, 14, 14])\n",
      "['6' '4' '9' '+' '8' '6' '4' '<eos>' '<pad>' '<pad>']\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "# TODO: your code here\n",
    "tokens = torch.tensor(pad_or_trim(tokenize_and_encode(q),10))\n",
    "padding_mask = generate_padding_mask(tokens, vocab[\"<pad>\"])\n",
    "print(tokens)\n",
    "print(decode_tokens(tokens))\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More tests\n",
    "assert list(generate_padding_mask(torch.tensor(pad_or_trim(tokenize_and_encode(\"1+1\"), 8)), vocab['<pad>'])) == [False]*4 + [True]*4, \"Something is wrong with generate_padding_mask\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) How will this mask be used by a transformer?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below takes the first batch of data from the training set, and it generates a shifted version of the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['9' '3' '6' '<eos>' '<pad>']\n",
      " ['8' '3' '1' '<eos>' '<pad>']\n",
      " ['1' '4' '7' '0' '<eos>']\n",
      " ['5' '6' '5' '<eos>' '<pad>']\n",
      " ['-' '2' '0' '0' '<eos>']]\n",
      "[['<bos>' '9' '3' '6' '<eos>']\n",
      " ['<bos>' '8' '3' '1' '<eos>']\n",
      " ['<bos>' '1' '4' '7' '0']\n",
      " ['<bos>' '5' '6' '5' '<eos>']\n",
      " ['<bos>' '-' '2' '0' '0']]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n",
    "y_prev = torch.cat((bos, y[:,:-1]), axis=1)\n",
    "\n",
    "# print the first five samples\n",
    "print(decode_tokens(y)[:5])\n",
    "print(decode_tokens(y_prev)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Look at the values for the example above. What is `y_prev` used for during training of a transformer model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is used during training of a transformer model for autoregressive tasks like language modeling. It serves as the input for the current time step, allowing the model to predict the next token based on the context of the previous tokens.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Why do some rows of `y_prev` end in `'<eos>'`, but not all? Is this a problem?<span style=\"float:right\"> (1 point)</span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the varying sequence length. It is not a problem during training transformers are suited for handling sequences of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below illustrates what the output of `generate_square_subsequent_mask` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "square_subsequent_mask = generate_square_subsequent_mask(y.shape[1])\n",
    "\n",
    "print(square_subsequent_mask.shape)\n",
    "print(square_subsequent_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) How and why should this mask be used? State your answer in terms of `x`,  `y` and/or `y_prev`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square_subsequent_mask is used when computing self-attention within the transformer model, ensuring that tokens in y or y_prev can attend only to previous tokens and not to future tokens (maintaining causality). It's applied during both training and sequence generation to prevent information from future tokens (e.g., in x or y) from affecting the current token's prediction.\n",
    "\r\n",
    "\r\n",
    "\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Give an example where it could make sense to use a different mask in a transformer network, instead of the `square_subsequent_mask`?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different mask, allowing bidirectional attention, might be used in tasks like Named Entity Recognition (NER) where tokens need to consider both past and future context for accurate predictions. The standard square_subsequent_mask enforces unidirectional attention and is not suitable for such tasks.\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Our discrete vocabulary is not suitable as the input for a transformer. We need an embedding function to map our input vocabulary to a continuous, high-dimensional space.\n",
    "\n",
    "We will use the `torch.nn.Embedding` class to for this. As you can read in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), this class maps each token in our vocabulary to a specific point in embedding space, its embedding vector. We will use this embedding vector as the input features for the next layer of our model.\n",
    "\n",
    "The parameters of the embedding are trainable: the embedding vector of each token is optimized along with the rest of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Define an embedding that maps our vocabulary to a 5-dimensional space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:38:54.748336Z",
     "start_time": "2023-10-06T13:38:54.670405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(16, 5)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your code here.\n",
    "embedding = torch.nn.Embedding(len(vocab), 5)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the embedding to some sequences from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:39:05.036450Z",
     "start_time": "2023-10-06T13:39:04.954051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 11,  4,  0,  4,  6,  6, 13, 14],\n",
      "        [ 7,  7,  8,  0,  4,  9,  7, 13, 14],\n",
      "        [ 9,  3,  5,  0,  9,  7,  9, 13, 14]])\n",
      "tensor([[[ 0.4096, -0.3713,  0.3825,  0.7601,  1.7625],\n",
      "         [-0.5651,  1.2649,  1.3879,  1.3397,  1.9781],\n",
      "         [ 1.8177,  0.1201,  1.9627, -2.3860,  0.6625],\n",
      "         [-2.2586, -1.1869, -0.0048,  0.0446,  0.4557],\n",
      "         [ 1.8177,  0.1201,  1.9627, -2.3860,  0.6625],\n",
      "         [ 0.6655,  0.0476, -1.6833, -0.5339, -0.5533],\n",
      "         [ 0.6655,  0.0476, -1.6833, -0.5339, -0.5533],\n",
      "         [ 0.8213, -0.5407, -0.2884,  0.7718, -0.1354],\n",
      "         [ 1.0419, -0.4233, -1.3558, -0.2121, -0.4962]],\n",
      "\n",
      "        [[ 1.3510, -0.7392, -0.4577, -1.4039, -0.8813],\n",
      "         [ 1.3510, -0.7392, -0.4577, -1.4039, -0.8813],\n",
      "         [ 0.4096, -0.3713,  0.3825,  0.7601,  1.7625],\n",
      "         [-2.2586, -1.1869, -0.0048,  0.0446,  0.4557],\n",
      "         [ 1.8177,  0.1201,  1.9627, -2.3860,  0.6625],\n",
      "         [-1.0152,  1.0581,  0.4677, -0.8905,  0.7418],\n",
      "         [ 1.3510, -0.7392, -0.4577, -1.4039, -0.8813],\n",
      "         [ 0.8213, -0.5407, -0.2884,  0.7718, -0.1354],\n",
      "         [ 1.0419, -0.4233, -1.3558, -0.2121, -0.4962]],\n",
      "\n",
      "        [[-1.0152,  1.0581,  0.4677, -0.8905,  0.7418],\n",
      "         [ 2.2560, -0.1767,  1.4696, -1.0919,  0.3510],\n",
      "         [-0.9733, -0.4913, -1.1409, -0.4935, -1.3168],\n",
      "         [-2.2586, -1.1869, -0.0048,  0.0446,  0.4557],\n",
      "         [-1.0152,  1.0581,  0.4677, -0.8905,  0.7418],\n",
      "         [ 1.3510, -0.7392, -0.4577, -1.4039, -0.8813],\n",
      "         [-1.0152,  1.0581,  0.4677, -0.8905,  0.7418],\n",
      "         [ 0.8213, -0.5407, -0.2884,  0.7718, -0.1354],\n",
      "         [ 1.0419, -0.4233, -1.3558, -0.2121, -0.4962]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 9])\n",
      "torch.Size([3, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "# take the first batch\n",
    "x, y = next(iter(train_loader))\n",
    "# take three samples\n",
    "x = x[:3]\n",
    "# print the shapes\n",
    "print(x)\n",
    "print(embedding(x))\n",
    "print(x.shape)\n",
    "print(embedding(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Explain the output shape.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding transform each element of x to a 5-d tensor. Since x has shape (3, 9), the output has shape (3, 9, 5), where 3 are the number of training samples, 9 the number of elements in each sample, 5 are the dimension for each element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the embedding vectors, or the dimensionality of the embedding space, does not depend on the number of tokens in our vocabulary. We are free to choose an embedding size that fits our problem.\n",
    "\n",
    "For example, let's try an embedding with 2 dimensions, and plot the initial embedding for the tokens in our vocabulary.\n",
    "\n",
    "**(i) Create an embedding with 2 dimensions and plot the embedding for all tokens.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T13:42:14.848489Z",
     "start_time": "2023-10-06T13:42:14.602590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Your code here.\n",
    "embedding = torch.nn.Embedding(len(vocab), 2)\n",
    "\n",
    "# embed all tokens of our vocabulary\n",
    "x = torch.arange(len(vocab))\n",
    "emb = embedding(x).detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(emb[:, 0], emb[:, 1]);\n",
    "for i, token in enumerate(vocab.idx_to_token):\n",
    "    plt.annotate(token, (emb[i,0]+0.04, emb[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need to balance the complexity of our networks: a larger embedding will increase the number of parameters in our model, but increase the risk of overfitting.\n",
    "\n",
    "**(j) Would this 2-dimensional embedding space be large enough for our problem?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the complexity of our problem, a 2-dimension could work. But we can also use a larger embedding space, 16 for simulate hot encoding, or even larger to improve the model performance.\n",
    "#todo check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using an embedding, we could also use a simple one-hot encoding to map the words in the vocabulary to feature vectors. However, practical applications of natural language processing never do this. Why not?\n",
    "\n",
    "**(k) Explain the practical advantage of embeddings over one-hot encoding.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical application of NLP they never do that because their vocabulary is too large and the one-hot encoding would be too sparse. The embedding can represent a large vocabulary in a smaller dense vector space, and it can be also trained to represent the similarity between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 `torch.nn.Transformer` (8 points)\n",
    "\n",
    "<div style=\"float: left\"><a href=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\"><img src=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\" width=\"300\"></a></div>\n",
    "\n",
    "We now have all required inputs for our transformer.\n",
    "\n",
    "Consult the documentation for the [`torch.nn.Transformer`](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) class of PyTorch. This class implements a full Transformer as described in [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf), the paper that introduced this architecture.\n",
    "\n",
    "The `Transformer` class implements the main part of the of the Transformer architecture, shown highlighted in the image on the left (see also Fig. 1 in \"Attention Is All You Need\").\n",
    "\n",
    "For a given input sequence, it applies one or more encoder layers, followed by one or more decoder layers, to compute an output sequence that we can then process further.\n",
    "\n",
    "Because the `Transformer` class takes care of most of the complicated parts of the model, we can concentrate providing the inputs and outputs: the grayed-out areas in the image.\n",
    "\n",
    "Check out the parameters for the `Transformer` class and the inputs and outputs of its `forward` function.\n",
    "<br style=\"clear: both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Which parameter of the `Transformer` class should we base on our embedding?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Given fixed input and output dimensions, which parameters of the `Transformer` can we use to change the complexity of our network?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) When using the `Transformer` class, where should we use the masks that we defined earlier?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Complete the code for the TransformerNetwork.<span style=\"float:right\"> (5 points)</span>**\n",
    "\n",
    "Construct a network with the following architecture (see the image in the previous section for an overview):\n",
    "1. An embedding layer that embeds the input tokens into a space of size `dim_hidden`.\n",
    "2. A dropout layer (not shown in the image).\n",
    "3. A [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) with the specified parameters (`dim_hidden`, `num_heads`, `num_layers`, `dim_feedforward`, and `dropout`).<br>Note: you will need to pass `batch_first=True`, to indicate that the first dimension runs over the batch and not over the sequence.\n",
    "4. A final linear prediction layer that takes the output of the transformer to `dim_vocab` possible classes.\n",
    "\n",
    "Don't worry about positional encoding for now, we will add that later.\n",
    "\n",
    "The `forward` function should generate the appropriate masks and combine the layers defined in `__init__` to compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_vocab=len(vocab), padding_token=vocab['<pad>'],\n",
    "                 num_layers=2, num_heads=4, dim_hidden=64, dim_feedforward=64,\n",
    "                 dropout=0.01, positional_encoding=False):\n",
    "        super().__init__()\n",
    "        self.padding_token = padding_token\n",
    "        # TODO: Your code here.\n",
    "        self.embedding    = ...\n",
    "        self.dropout      = ...\n",
    "        self.transformer  = ...\n",
    "        self.predict      = ...\n",
    "        if positional_encoding:\n",
    "            self.pos_encoding = ... # Fill this in later\n",
    "        else:\n",
    "            self.pos_encoding = torch.nn.Identity()\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # TODO: Your code here.\n",
    "        # Combine self.embedding, self.dropout, self.transformer, self.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Try the transformer with an example batch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerNetwork' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerNetwork\u001b[49m(dim_feedforward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m72\u001b[39m)\n\u001b[1;32m      2\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[1;32m      3\u001b[0m bos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<bos>\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mexpand(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerNetwork' is not defined"
     ]
    }
   ],
   "source": [
    "net = TransformerNetwork(dim_feedforward=72)\n",
    "x, y = next(iter(train_loader))\n",
    "bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n",
    "y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n",
    "\n",
    "print('x.shape', x.shape)\n",
    "print('y.shape', y.shape)\n",
    "print('y_prev.shape', y_prev.shape)\n",
    "\n",
    "y_pred = net(x, y_prev)\n",
    "print('y_pred.shape', y_pred.shape)\n",
    "\n",
    "# check the shape against what we expected\n",
    "np.testing.assert_equal(list(y_pred.shape), [y.shape[0], y.shape[1], len(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert these predictions to tokens (but they're obviously random):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode_tokens(torch.argmax(y_pred, dim=2))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the transformer is defined correctly\n",
    "assert isinstance(net.embedding, torch.nn.Embedding)\n",
    "assert isinstance(net.dropout, torch.nn.Dropout)\n",
    "assert isinstance(net.transformer, torch.nn.Transformer)\n",
    "assert isinstance(net.predict, torch.nn.Linear)\n",
    "# Check parameters of transformer\n",
    "assert net.transformer.d_model == 64\n",
    "assert net.transformer.nhead == 4\n",
    "assert net.transformer.batch_first == True\n",
    "assert net.transformer.encoder.num_layers == 2\n",
    "assert net.transformer.decoder.num_layers == 2\n",
    "assert net.transformer.encoder.layers[0].linear1.out_features == 72\n",
    "assert net.dropout.p == 0.01\n",
    "assert net.transformer.encoder.layers[0].dropout.p == 0.01\n",
    "# Check that the forward function behaves correctly\n",
    "net.train(False)\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(torch.cat((x,torch.tensor(vocab['<pad>']).expand(x.shape[0], 5)), axis=1), y_prev), atol=1e-5)), \\\n",
    "       \"Adding padding to x should not affect the output of the network. Check src_key_padding_mask and memory_key_padding_mask. The former controls self attention to padding tokens in the encoder, the latter controls cross attention from decoder to encoder.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(x, torch.cat((y_prev,torch.tensor(vocab['<pad>']).expand(y.shape[0], 5)), axis=1))[:,:-5], atol=1e-5)), \\\n",
    "       \"Adding padding to y should not affect the output of the network. Check tgt_key_padding_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev)[:,:2], \\\n",
    "            net(x, y_prev[:,:2]), atol=1e-5)), \\\n",
    "       \"The presence of later tokens in y should not affect the output for earlier tokens. Check tgt_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(torch.flip(x, [1]), y_prev), atol=1e-5)), \\\n",
    "       \"Order of x should not matter for a transformer network. Check src_mask.\"\n",
    "assert not torch.all(torch.isclose( \\\n",
    "            net(x, torch.flip(y_prev, [1])), \\\n",
    "            torch.flip(net(x, y_prev), [1]), atol=1e-5)), \\\n",
    "       \"Order of y should matter for a transformer network. Check tgt_mask.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Training (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will base the training code on last week's code. A complication in computing the loss and accuracy are the padding tokens. So, before we work on the training loop itself, we need to update the `accuracy` function so it ingores these `<pad>` tokens. Let's do this in a generic way\n",
    "\n",
    "**(a) Copy the `accuracy` function from last week, and add a parameter `ignore_index`. The tokens with `y == ignore_index` should be ignored.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: you can select elements from a tensor with `some_tensor[include]` where `include` is a tensor of booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y, ignore_index=None):\n",
    "    # Computes the mean accuracy.\n",
    "    # y_hat: raw network output (before sigmoid or softmax)\n",
    "    #        shape (samples, classes)\n",
    "    # y:     shape (samples)\n",
    "    #ignore_index: ignore tokens equal to ignore_index\n",
    "    ignore_indices = torch.where(y!=ignore_index)\n",
    "    y_hat = y_hat[ignore_indices]\n",
    "    y = y[ignore_indices]\n",
    "    if y_hat.shape[1] == 1:\n",
    "        # binary classification\n",
    "        y_hat = (y_hat[:, 0] > 0).to(y.dtype)\n",
    "    else:\n",
    "        # multi-class classification\n",
    "        y_hat = torch.argmax(y_hat, axis=1).to(y.dtype)\n",
    "    correct = (y_hat == y).to(torch.float32)\n",
    "    return torch.mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the accuracy function.\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 1) == 2/3\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 2) == 1\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 3) == 3/4\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([2,2,1,2]), 2) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Write a training loop for the transformer model.<span style=\"float:right\"> (4 points)</span>**\n",
    "\n",
    "See last week's assignment for inspiration.\n",
    "The code is mostly the same with the following changes:\n",
    " * The cross-entropy loss function and accuracy should ignore all `<pad>` tokens. (Use `ignore_index`, see the [documentation of CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).)\n",
    " * The network expects `y_prev` as an extra input.\n",
    " * The output of the network contains a batch of N samples, with maximum length L, and gives logits over C classes, so it has size (N,L,C). But `CrossEntropyLoss` and `accuracy` expect a tensor of size (N,C,L). You can use [torch.Tensor.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html) to change the output to the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_loaders, epochs=100, lr=0.001, device=device):\n",
    "    \"\"\"\n",
    "    Trains the model net with data from the data_loaders['train'] and data_loaders['val'].\n",
    "    \"\"\"\n",
    "    net = net.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    animator = d2l.Animator(xlabel='epoch',\n",
    "                            legend=['train loss', 'train acc', 'validation loss', 'validation acc'],\n",
    "                            figsize=(10, 5))\n",
    "\n",
    "    timer = {'train': d2l.Timer(), 'val': d2l.Timer()}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # monitor loss, accuracy, number of samples\n",
    "        metrics = {'train': d2l.Accumulator(3), 'val': d2l.Accumulator(3)}\n",
    "\n",
    "        for phase in ('train', 'val'):\n",
    "            # switch network to train/eval mode\n",
    "            net.train(phase == 'train')\n",
    "\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):\n",
    "                timer[phase].start()\n",
    "\n",
    "                # move to device\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # compute prediction\n",
    "                #NETWORK EXPECTS Y_PREV AS AN EXTRA INPUT\n",
    "                bos = torch.tensor(vocab['<bos>']).expand(y.shape[0], 1)\n",
    "                y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n",
    "                y_hat = net(x, y_prev)\n",
    "\n",
    "                #PERMUTE FROM (N,L,C) to (N,C,L)\n",
    "                y_hat = torch.permute(0,2,1)\n",
    "                \n",
    "                if y_hat.shape[1] == 1:\n",
    "                    # compute binary cross-entropy loss\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()(y_hat[:, 0], y.to(torch.float))\n",
    "                else:\n",
    "                    # compute cross-entropy loss\n",
    "                    #IGNORE <pad> index 14\n",
    "                    loss = torch.nn.CrossEntropyLoss()(y_hat, y, ignore_index=14)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # compute gradients and update weights\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                metrics[phase].add(loss * x.shape[0],\n",
    "                                   accuracy(y_hat, y) * x.shape[0],\n",
    "                                   x.shape[0])\n",
    "\n",
    "                timer[phase].stop()\n",
    "\n",
    "        animator.add(epoch + 1,\n",
    "            (metrics['train'][0] / metrics['train'][2],\n",
    "             metrics['train'][1] / metrics['train'][2],\n",
    "             metrics['val'][0] / metrics['val'][2],\n",
    "             metrics['val'][1] / metrics['val'][2]))\n",
    "\n",
    "    train_loss = metrics['train'][0] / metrics['train'][2]\n",
    "    train_acc  = metrics['train'][1] / metrics['train'][2]\n",
    "    val_loss   = metrics['val'][0] / metrics['val'][2]\n",
    "    val_acc    = metrics['val'][1] / metrics['val'][2]\n",
    "    examples_per_sec = metrics['train'][2] * epochs / timer['train'].sum()\n",
    "    \n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'val loss {val_loss:.3f}, val acc {val_acc:.3f}')\n",
    "    print(f'{examples_per_sec:.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train a transformer network. Use 100 epochs with a learning of 0.001<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TransformerNetwork(dim_feedforward=72)\n",
    "train(net, data_loaders, epochs=100, lr=0.001, device=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Briefly discuss the results. Has the training converged? Is this a good calculator?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, q, a):\n",
    "    # Run net to predict the output given the input `q` and y_prev based on `a`.\n",
    "    # Return predicted y\n",
    "    with torch.no_grad():\n",
    "        # Tokenize and encode the input strings\n",
    "        q_encoded = tokenize_and_encode(q)\n",
    "        a_encoded = tokenize_and_encode(a)\n",
    "\n",
    "        # Convert the encoded tokens to tensors\n",
    "        q_tensor = torch.tensor(pad_or_trim(q_encoded, max_question_length, vocab))\n",
    "        a_tensor = torch.tensor(pad_or_trim(a_encoded, max_answer_length, vocab))\n",
    "\n",
    "        # Move tensors to the device\n",
    "        q_tensor = q_tensor.to(device)\n",
    "        a_tensor = a_tensor.to(device)\n",
    "\n",
    "        # Expand dimensions to add batch size (assuming batch size = 1)\n",
    "        q_tensor = q_tensor.unsqueeze(0)\n",
    "        a_tensor = a_tensor.unsqueeze(0)\n",
    "\n",
    "        # Predict the output\n",
    "        y_pred = net(q_tensor, a_tensor)\n",
    "\n",
    "for src, tgt in [('123+123', '246'), ('321+321', '642')]:\n",
    "    print(f'For {src}={tgt}')\n",
    "    y_pred = predict(net, src, tgt)\n",
    "    print('  y_pred[0]', y_pred[0])\n",
    "    print('  encoded', torch.argmax(y_pred, dim=-1))\n",
    "    print('  tokens', decode_tokens(torch.argmax(y_pred, dim=-1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Compare the predictions for the first element of y with the two different inputs. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Does the validation accuracy estimate how often the model is able to answers formulas correctly? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) If the forward function takes the shifted output `y_prev` as input, how can we use it if we don't know the output yet?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Positional encoding (5 points)\n",
    "\n",
    "We did not yet include positional encoding in the network.\n",
    "PyTorch does not include such an encoder, so here we copied the code from the book (slightly modified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, max_len=1000):\n",
    "        super().__init__()\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X + self.P[:, :X.shape[1], :].to(X.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Add positional encoding to the TransformerModel.<span style=\"float:right\"> (point given in earlier question)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: See over there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Construct and train a network with positional encoding<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your answer here\n",
    "net_pos = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How does the performance of a model with positional encoding compare to a model without?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compare the predictions for the first element of y with what you found earlier. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Explain in your own words why positional encoding is used in transformer networks.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Look at the learning curve. Can you suggest a way to improve the model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Optional: if time permits, try to train an even better model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Predicting for new samples (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting an output given a new sample requires an appropriate search algorithm (see [d2l chapter 10.8](https://d2l.ai/chapter_recurrent-modern/beam-search.html)). Here, we will implement the simplest form: a greedy search algorithm that selects the token with the highest probability at each time step.\n",
    "\n",
    "**(a) Describe this search strategy in pseudo-code.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Implement a greedy search function to predict a sequence using `net_pos`.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_greedy(net, src, length):\n",
    "    # predict an output sequence of the given (maximum) length given input string src\n",
    "    with torch.no_grad():\n",
    "        # TODO: Your code here.\n",
    "        pass\n",
    "\n",
    "predicted_sequence = predict_greedy(net_pos, '123+123', 6)\n",
    "print(decode_tokens(predicted_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Does this search strategy give a high-quality prediction? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) What alternative search strategy could we use to improve the predictions? Why would this help?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Discussion (4 points)\n",
    "\n",
    "Last week, we looked at recurrent neural networks such as the LSTM. Both recurrent neural networks and transformers work with sequences, but in recent years the transformer has become more popular than the recurrent models.\n",
    "\n",
    "**(a) An advantage of transformers over recurrent neural is that they can be faster to train. Why is that?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Does this advantage also hold when predicting outputs for new sequences? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Why is positional encoding often used in transformers, but not in convolutional or recurrent neural networks?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a recurrent neural network makes it very suitable for online predictions, such as real-time translation, because it only depends on prior inputs. You can design an architecture where the RNN produces an output token for every input token given to it, and it can produce that output without having to wait for the rest of the input.\n",
    "\n",
    "Note: 'online' means producing outputs continuously as new input comes in, as opposed to collecting a full dataset and analyzing it afterwards, it has nothing to do with the internet.\n",
    "\n",
    "**(d) How would a transformer work in an online application? Do you need to change the architecture?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 47 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 3c66915 / 2023-10-04</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
