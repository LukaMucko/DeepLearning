{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seventh assignment for the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "-----\n",
    "\n",
    "**Names: Luka Mucko, Luca Poli**\n",
    "\n",
    "**Group: 46**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "For this assignment we are doing things a bit different.\n",
    "* Your task is to reproduce the paper [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks;      Jonathan Frankle, Michael Carbin](https://arxiv.org/pdf/1803.03635.pdf).\n",
    "* Try to follow the experimental settings in that paper, unless there is a reason to deviate.\n",
    "* If the paper is not clear on some details, make a reasonable choice yourself, and motivate that choice.\n",
    "* You will have 3 weeks to work on this assignment.\n",
    "* Be aware that this assignment will take more time than previous ones. It is ok if you do not completely finish it.\n",
    "* We will *not* be providing you with much code. You will have to implement many things yourself.\n",
    "* You may freely use code from earlier weeks, and from the d2l books. Please add a comment to reference the original source.\n",
    "* You may *not* use implementations of the paper you find online.\n",
    "\n",
    "**Tips and hints**\n",
    "* It is allowed and recommended to use more than just this notebook. Make separate python files for a library of functions, and for training and analyses.\n",
    "* If you like working with jupyter notebooks: make a separate notebook for trying things out, and keep this one clean.\n",
    "* Use checkpoint files before and during training.\n",
    "* In the notebook only display and discuss these results.\n",
    "* You may add new cells to this notebook as needed.\n",
    "* While the task is to reproduce parts of a paper, the big picture is more important than the exact details.\n",
    "* It is allowed to discuss the assignment with other groups, but try not to spoil too much.\n",
    "* If you get stuck, contact the teachers via discord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Required software\n",
    "\n",
    "If you need to import any additional libraries, add them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-11-06T18:12:42.020788Z",
     "end_time": "2023-11-06T18:12:42.247369Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from d2l import torch as d2l\n",
    "from dl_assignment_7_common import *  # Your functions should go here if you want to use them from scripts\n",
    "\n",
    "device = d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 7.1 The paper (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**(a) Read sections 1, 2, and 5 of the paper [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks; by Jonathan Frankle, Michael Carbin](https://arxiv.org/pdf/1803.03635.pdf)**\n",
    "\n",
    "We will refer to this as the \"LTH paper\" from now on, or just \"the paper\".\n",
    "To answer some later questions you will also need to look at other sections, and search through the appendices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**(b) In your own words, briefly explain the key message of the paper.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "Note: \"briefly\" means: a few sentences at most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The key message of the paper, is that in large neural network there exist smaller subnetwork (called winning tickets) that can be trained in isolation to achieve comparable performance to the original network (or even better) in less epochs. The winning tickets are found with an iterative process of pruning and retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 7.2 Models and datasets (9 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**(a) What neural network architectures are used in the paper?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the paper the network architectures used are: Lenet, Conv-2, Conv-4, Conv-6, Resnet-18 and VGG-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To keep things simple, we will start with a simple architecture, corresponding to what the paper calls `Lenet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**(b) Define a function that constructs a `Lenet` network using PyTorch.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "Copy these definitions to `dl_assignment_7_common.py`.\n",
    "\n",
    "Hint: see Figure 2.\n",
    "\n",
    "Note: the LTH paper is not entirely clear about this, but the convolution layers use `padding='same'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "start_time": "2023-11-06T17:11:34.988903Z",
     "end_time": "2023-11-06T17:11:37.170473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenet: 266610 parameters\n",
      "resnet18: 282938 parameters\n",
      "vgg19: 20029514 parameters\n"
     ]
    }
   ],
   "source": [
    "for net_type in [\"lenet\", \"resnet18\", \"vgg19\"]:\n",
    "    print(f\"{net_type}: {sum(p.numel() for p in create_network(net_type).parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (1): ReLU()\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (3): ReLU()\n  (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (6): ReLU()\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (8): ReLU()\n  (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (11): ReLU()\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (13): ReLU()\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (15): ReLU()\n  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (17): ReLU()\n  (18): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (20): ReLU()\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (22): ReLU()\n  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (24): ReLU()\n  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (26): ReLU()\n  (27): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (29): ReLU()\n  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (31): ReLU()\n  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (33): ReLU()\n  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n  (35): ReLU()\n  (36): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n  (37): Flatten(start_dim=1, end_dim=-1)\n  (38): Linear(in_features=512, out_features=10, bias=True)\n)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_network(\"vgg19\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T18:15:44.221300Z",
     "end_time": "2023-11-06T18:15:44.843295Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): ReLU(inplace=True)\n    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (26): ReLU(inplace=True)\n    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): ReLU(inplace=True)\n    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (33): ReLU(inplace=True)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): ReLU(inplace=True)\n    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "models.vgg19()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T18:26:23.671231Z",
     "end_time": "2023-11-06T18:26:28.472268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**(c) Define a function that can construct a network given the architecture name.**\n",
    "\n",
    "To keep the code as generic as possible, we can make function\n",
    "\n",
    "Move the function below to `dl_assignment_7_common.py`, and don't forget to remove it here."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will do all our experiments with two datasets: MNIST and FashionMNIST\n",
    "\n",
    "**(d) Are these datasets also used in the papers?<span style=\"float:right\"> (1 point)</span>**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the paper they use MNIST and CIFAR10, so the dataset FashionMNIST is not used the paper."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(e) Define a function that loads a dataset given the dataset name.<span style=\"float:right\"> (3 points)</span>**\n",
    "\n",
    "Hint: Standard datasets such as MNIST and CIFAR10 are available in the [torchvision](https://pytorch.org/vision/stable/datasets.html#image-classification) library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: Function implemented as: `TODO` in `dl_assignment_7_common.py`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(f) Most of these datasets come with a predefined train/test split. Is this used in the LTH paper? If so, update the dataset loader to return a pair `(trainset, testset)`.<span style=\"float:right\"> (1 point)</span>**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the paper they use the predefined train/test split, which is 60000 training images and 10000 test images for MNIST and 50000 training images and 10000 test images for CIFAR10."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(g) Does the LTH paper use a validation set? If so, update the dataset loader to return `(train_dataset, validation_dataset, test_dataset)`.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: [`random_split`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split), and/or see assignment 2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the paper they have sampled a validation set of 5000 images from the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: 55000 training images, 5000 validation images, 10000 test images\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR10: 45000 training images, 5000 validation images, 10000 test images\n",
      "FashionMNIST: 55000 training images, 5000 validation images, 10000 test images\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"MNIST\", \"CIFAR10\" ,\"FashionMNIST\"]:\n",
    "    train_data, validation_data, test_data = get_dataloaders(dataset)\n",
    "    print(f\"{dataset}: {len(train_data.dataset)} training images, {len(validation_data.dataset)} validation images, {len(test_data.dataset)} test images\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T15:30:03.812182Z",
     "end_time": "2023-11-06T15:30:25.671877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7.3 Training (12 points)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(a) What optimization algorithm is used in the paper? What values are used for the hyperparameters?<span style=\"float:right\"> (1.5 points)</span>**\n",
    "\n",
    "If you are unable to find the values used for some of the hyperparameters, use reasonable default values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ADAM was used as the optimizer for Lenet and Conv networks, for resnet and VGG SGD with momentum was used. <br/>\n",
    "They use learning rates $3\\cdot 10^{-4}$, $1.2\\cdot 10^{-3}$, $2\\cdot 10^{-4}$ for Lenet and Conv networks. <br/>\n",
    "For sgd with momentum they used learning rates 0.1-0.01-0.001 and momentum 0.9\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(b) Implement an evaluation function, that evaluates a model on a validation or test set (passed as an argument).<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "The function should return loss and accuracy.\n",
    "\n",
    "Hints: the book defines a function for this that you may use (see assignment 3)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(c) Implement a training loop.<span style=\"float:right\"> (4 points)</span>**\n",
    "\n",
    "Make sure that the network parameters are saved to a file before and during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because you will be doing many experiments, it would be a shame to have to re-run them when you reload the notebook. A better solution is to save model checkpoints.\n",
    "See [the tutorial on saving and loading model parameters](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) for how to implement this in PyTorch.\n",
    "\n",
    "**(d) Change the training function so that it saves the model at the start and at the end of training.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: Saving a model requires a filename. Because you will be running many experiments, come up with a descriptive naming convention and/or directory structure. Example: `path = f\"checkpoints/model-{arch}-{dataset}-{run}-{phase_of_the_moon}-{iteration}.pth\"`.\n",
    "\n",
    "Hint 2: it is easier to save the whole model, see the bottom of the tutorial."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(e) Train a simple network on a simple dataset.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "You may want to create a new python script (`simple_training.py`), and just load the trained network here instead."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# simple_training_net1, stats1 = load_network(\"simple_training_1\", 0.01, \"adam\")\n",
    "# print_plot_results(stats1, \"simple_training_1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T15:30:25.687056Z",
     "end_time": "2023-11-06T15:30:26.286954Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Does the training converge? How well does your network perform?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Re-train the same network, with the *same* initial weights. Are the results *exactly* the same?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# simple_training_net2, stats2 = load_network(\"simple_training_2\", 0.01, \"adam\")\n",
    "# print_plot_results(stats2, \"simple_training_2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T15:30:26.289951Z",
     "end_time": "2023-11-06T15:30:26.598278Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) The LTH paper uses a variant of 'early-stopping'. How is this done? Implement it in your training loop.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: A simple way to keep track of the best model is to create a model checkpoint in a file `\"checkpoints/model-...-best.pth\"`.\n",
    "\n",
    "Hint 2: It is okay to compute the validation scores less often, this can speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper they use a retroactive variant of early stopping. They train a network for a fixed number of iterations, and then, with the validations performance they select the moment with the best network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Pruning (13 points)\n",
    "\n",
    "Next up, you should implement pruning. Starting with the one-shot pruning method.\n",
    "\n",
    "Hint: \n",
    "Pruning is implemented already in PyTorch, in the module [torch.utils.prune](https://pytorch.org/docs/stable/nn.html#module-torch.nn.utils).\n",
    "The pruning method used in the LTH paper corresponds is called [l1_unstructured](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.l1_unstructured.html) in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) The PyTorch pruning function accepts an amount to prune. Is that the amount of weights to set to 0 or the amount to keep nonzero? Is the paper using the same?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount in the pytorch pruning function is the amount to set to 0. In the paper they indicate whit p% the amount to prune (like in pytorch) but the result are expressed with Pm which is the percentage of weights remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Should all parameters be pruned or only weights? Is the pruning rate the same for all types of layers?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning biases can lead to difficulties in training the network effectively, so we should only prune weights (like in the paper). The pruning rate change for different types of layers (for fully connected and convolutional layers), but it does not depend on the layer position (except for the connections to the last layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Define a function to prune a network as used in the LTH paper. It should take an amount to prune as an argument.<span style=\"float:right\"> (3 points)</span>**\n",
    "\n",
    "Hint: for a `Sequential` layer, you can access the layers as `net.children()`. For a layer, you can use `isinstance(layer, torch.nn.Linear)` to check if it is a linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Function implemented as: `TODO` in `dl_assignment_7_common.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Check your pruning function on a very simple neural network. Print the layer weights before and after pruning to make sure you understand what is going on.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: the PyTorch pruning functions do not change the trainable parameters, rather they set `module.weight` to `weight_orig * weight_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net: lenet Before pruning: 266610 After pruning: 133760 Pruned: 49.83\n",
      "Net: conv2 Before pruning: 4301642 After pruning: 2151786 Pruned: 49.98\n",
      "Net: conv4 Before pruning: 2425930 After pruning: 1214058 Pruned: 49.95\n",
      "Net: resnet18 Before pruning: 282938 After pruning: 142306 Pruned: 49.7\n",
      "Net: vgg19 Before pruning: 20029514 After pruning: 10020074 Pruned: 49.97\n"
     ]
    }
   ],
   "source": [
    "for net_type in [\"lenet\", \"resnet18\", \"vgg19\"]:\n",
    "    net = create_network(net_type)\n",
    "    params_before = sum(p.numel() for p in net.parameters())\n",
    "    prune_network(net, net_type, 0.5)\n",
    "    params_after = params_before - sum(torch.sum(b == 0) for b in net.buffers()).item()\n",
    "    print(\"Net: \" + net_type + \" Before pruning: \" + str(params_before) + \" After pruning: \" + str(params_after) +\n",
    "          \" Pruned: \" + str(round(abs(1-(params_after / params_before))*100, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:30.243997Z",
     "end_time": "2023-11-02T18:53:39.596519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Parameter containing:\n",
      "tensor([[ 0.0145, -0.0049,  0.0242,  ..., -0.0266, -0.0234,  0.0284],\n",
      "        [-0.0113,  0.0273,  0.0011,  ...,  0.0241, -0.0180, -0.0321],\n",
      "        [ 0.0286,  0.0233,  0.0346,  ...,  0.0018, -0.0036, -0.0119],\n",
      "        ...,\n",
      "        [-0.0067,  0.0227,  0.0083,  ..., -0.0289,  0.0171,  0.0136],\n",
      "        [-0.0031,  0.0099, -0.0295,  ..., -0.0125, -0.0154, -0.0251],\n",
      "        [-0.0161, -0.0237,  0.0043,  ..., -0.0099,  0.0129, -0.0314]],\n",
      "       requires_grad=True)\n",
      "After: tensor([[ 0.0145, -0.0000,  0.0242,  ..., -0.0266, -0.0234,  0.0284],\n",
      "        [-0.0113,  0.0273,  0.0000,  ...,  0.0241, -0.0180, -0.0321],\n",
      "        [ 0.0286,  0.0233,  0.0346,  ...,  0.0000, -0.0000, -0.0119],\n",
      "        ...,\n",
      "        [-0.0000,  0.0227,  0.0083,  ..., -0.0289,  0.0171,  0.0136],\n",
      "        [-0.0000,  0.0099, -0.0295,  ..., -0.0125, -0.0154, -0.0251],\n",
      "        [-0.0161, -0.0237,  0.0000,  ..., -0.0099,  0.0129, -0.0314]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net_type = \"lenet\"\n",
    "net = create_network(net_type)\n",
    "print(\"Before: \" + str(list(net.children())[0].weight))\n",
    "prune_network(net, net_type, 0.2)\n",
    "print(\"After: \" + str(list(net.children())[0].weight))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:39.612266Z",
     "end_time": "2023-11-02T18:53:40.100258Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53140\n"
     ]
    }
   ],
   "source": [
    "print(sum(torch.sum(b == 0) for b in net.buffers()).item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:40.100258Z",
     "end_time": "2023-11-02T18:53:40.449912Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Define a function that applies the pruning mask from a pruned network to another network of the same architecture.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "This function should only do pruning (so some weights become 0), other weights should remain the same.\n",
    "\n",
    "Hint: the pruning functions already generate and store pruning masks. You should be able to extract these from a pruned network.\n",
    "\n",
    "Hint 2: `custom_from_mask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Function implemented as: `TODO` in `dl_assignment_7_common.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Check your mask copy function on a very simple neural network. Check that only the pruning mask is copied.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Parameter containing:\n",
      "tensor([[-0.0075, -0.0188,  0.0311,  ..., -0.0330, -0.0178,  0.0280],\n",
      "        [ 0.0076,  0.0107,  0.0211,  ...,  0.0009, -0.0283,  0.0081],\n",
      "        [-0.0292, -0.0038,  0.0200,  ..., -0.0116, -0.0153,  0.0259],\n",
      "        ...,\n",
      "        [ 0.0324,  0.0086,  0.0115,  ...,  0.0066,  0.0319, -0.0295],\n",
      "        [-0.0009,  0.0058,  0.0264,  ..., -0.0131, -0.0017, -0.0036],\n",
      "        [ 0.0344,  0.0166, -0.0171,  ..., -0.0148, -0.0139, -0.0031]],\n",
      "       requires_grad=True)\n",
      "After: tensor([[-0.0075, -0.0188,  0.0311,  ..., -0.0000, -0.0000,  0.0280],\n",
      "        [ 0.0076,  0.0107,  0.0000,  ...,  0.0009, -0.0283,  0.0081],\n",
      "        [-0.0000, -0.0000,  0.0200,  ..., -0.0000, -0.0153,  0.0259],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0066,  0.0319, -0.0295],\n",
      "        [-0.0009,  0.0000,  0.0264,  ..., -0.0000, -0.0017, -0.0036],\n",
      "        [ 0.0344,  0.0166, -0.0171,  ..., -0.0148, -0.0139, -0.0031]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Before pruning: 266610 After pruning: 186900 Pruned: 29.9\n"
     ]
    }
   ],
   "source": [
    "net_type = \"lenet\"\n",
    "net_in = create_network(net_type)\n",
    "prune_network(net_in, net_type, 0.3)\n",
    "\n",
    "net_out = create_network(net_type)\n",
    "print(\"Before: \" + str(list(net_out.children())[0].weight))\n",
    "params_before = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "prune_network_from_mask(net_in, net_out)\n",
    "params_after = params_before - sum(torch.sum(b == 0) for b in net_out.buffers()).item()\n",
    "print(\"After: \" + str(list(net_out.children())[0].weight))\n",
    "\n",
    "print(\"Before pruning: \" + str(params_before) + \" After pruning: \" + str(params_after) +\n",
    "      \" Pruned: \" + str(round(abs(1-(params_after / params_before))*100, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:40.401917Z",
     "end_time": "2023-11-02T18:53:40.793639Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0.3"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pruned_params(net_out, params_before)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:40.793639Z",
     "end_time": "2023-11-02T18:53:41.050029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[('0.bias',\n  Parameter containing:\n  tensor([-2.3712e-03, -3.1045e-02,  3.4961e-02,  3.4814e-02, -9.7466e-03,\n          -3.5709e-02,  3.1714e-02,  4.4494e-03, -2.8082e-02,  2.3412e-02,\n           1.4089e-02,  6.9029e-03,  2.8267e-03, -1.2971e-02,  7.7887e-03,\n          -1.2163e-02, -3.3372e-02, -2.6097e-02, -3.5347e-02, -5.0010e-03,\n          -7.4490e-03,  4.7466e-03, -2.9549e-02, -4.7142e-03,  2.5709e-02,\n          -1.4715e-02, -1.1934e-02,  3.6295e-03, -4.1924e-03, -7.6258e-03,\n          -2.5322e-02,  1.3163e-02,  1.7379e-02, -8.1354e-03, -2.8293e-02,\n           1.9079e-02, -6.6706e-03,  3.0807e-02, -3.0795e-02,  1.1585e-02,\n           3.2486e-02, -2.8414e-02, -1.9498e-02,  2.8577e-02, -3.4625e-02,\n           9.1021e-03,  3.3017e-02,  3.3393e-02, -4.9070e-03, -1.6608e-02,\n           2.5116e-02, -3.0546e-02, -1.7663e-02, -1.2790e-02, -8.9883e-03,\n           2.4491e-02,  1.8514e-02, -1.3186e-02,  1.1886e-03,  1.3502e-02,\n          -2.2691e-02, -2.8571e-02, -1.9082e-02, -1.4495e-02, -1.6811e-02,\n           1.1039e-02,  2.5352e-02, -1.8517e-02, -9.0006e-03, -3.5642e-02,\n           3.2894e-02,  9.2797e-03,  1.9006e-02,  1.0432e-02, -2.7466e-03,\n          -1.3838e-02, -1.9435e-02, -3.1066e-03, -2.5427e-02, -3.5045e-02,\n           2.7121e-02,  2.2206e-02,  7.3354e-03,  3.4783e-02,  3.0313e-02,\n           3.1666e-03,  2.3743e-02,  1.3564e-02, -1.5327e-02, -2.6105e-02,\n           2.1600e-02, -9.8234e-03, -1.2989e-02, -1.9052e-02,  3.3118e-02,\n          -3.1063e-02,  1.9963e-02,  2.0565e-02, -1.0039e-02, -2.4911e-02,\n           7.5353e-03, -2.1685e-02, -2.6582e-02,  3.2400e-02,  2.5630e-02,\n           1.1148e-02,  1.2202e-02,  3.1430e-02, -1.3773e-02, -2.8901e-02,\n          -1.7030e-03, -3.5665e-02,  1.8824e-02,  2.7877e-02,  3.3591e-02,\n           2.4818e-02,  2.2165e-02,  3.7700e-05, -4.6854e-03,  1.2628e-02,\n           7.5118e-03,  3.5203e-02, -6.7909e-03,  2.0503e-02, -2.5615e-02,\n           1.1330e-02,  3.5429e-02, -1.6458e-02,  2.4243e-02, -1.9332e-02,\n           1.4447e-02, -1.6516e-02, -5.8014e-03,  1.5362e-02,  3.0141e-02,\n           1.3692e-02,  1.9567e-02, -5.0259e-04, -1.0404e-02, -3.1682e-03,\n          -2.5170e-03, -3.0756e-02, -8.3468e-03,  1.4632e-02,  2.7978e-02,\n           8.9850e-03, -3.4678e-02,  2.8209e-02,  3.8913e-03,  2.3181e-02,\n           2.3739e-02,  1.7543e-02, -7.4332e-03, -6.6335e-03,  3.4979e-02,\n          -1.4790e-03,  6.6506e-03, -2.9893e-02, -2.4822e-02, -1.8082e-02,\n           6.0771e-04,  2.0406e-02, -2.6587e-02, -5.7154e-03, -3.3980e-02,\n          -3.9924e-04, -3.0329e-02, -2.1715e-02, -9.7414e-03, -3.2137e-02,\n           2.0290e-03,  2.5165e-03, -9.3715e-03, -6.1505e-03,  3.4761e-04,\n          -2.2113e-02,  8.9323e-03,  4.8132e-03, -9.2062e-03, -1.7299e-02,\n           2.6397e-03,  1.2815e-02, -1.6425e-02, -2.3310e-02,  9.8644e-03,\n          -2.5195e-02, -1.2706e-02, -2.9144e-02, -1.0024e-02,  1.9785e-02,\n          -3.4652e-02, -1.2035e-02,  2.9253e-02,  5.6713e-04, -3.4042e-02,\n           3.0695e-02,  4.8554e-03, -1.8377e-02,  2.6592e-02, -3.3611e-04,\n          -1.4097e-03,  6.4571e-03, -2.2212e-02, -3.0404e-02, -2.0908e-02,\n          -3.2838e-02,  5.0152e-03, -2.4880e-02, -2.2671e-02,  1.3495e-02,\n          -6.0618e-03,  5.1486e-03,  1.4691e-02, -1.7265e-02,  2.2210e-02,\n           1.2528e-02, -1.8035e-02, -3.8367e-05, -3.4536e-02,  2.5173e-02,\n          -5.2516e-03,  3.3675e-02,  8.1718e-03,  2.7921e-02, -2.6792e-03,\n           2.1622e-03,  2.9951e-02,  3.0317e-02,  1.6930e-02,  2.1120e-02,\n          -3.2952e-03, -2.7082e-03, -1.7230e-02,  3.7312e-03, -2.6165e-02,\n           2.7825e-02, -1.4872e-02, -1.8010e-02,  2.8276e-02,  2.8606e-02,\n          -1.8566e-02, -3.0339e-02, -2.4240e-02,  8.8683e-03, -1.6546e-02,\n          -7.8597e-03,  4.8204e-03,  2.7994e-03, -1.7688e-02,  2.4621e-02,\n           3.6536e-03, -8.2270e-03,  1.0193e-02,  3.5460e-02,  1.5444e-02,\n           2.2039e-02,  5.6790e-03,  2.8153e-02, -2.0866e-02, -2.9223e-02,\n           2.5023e-02, -1.5238e-02,  3.2075e-02, -1.7153e-03, -1.2326e-02,\n          -3.0753e-02, -2.2955e-02, -2.5990e-02,  5.7655e-03, -1.7512e-02,\n           7.3934e-03,  1.2082e-03,  3.0597e-02, -2.4198e-02, -3.2305e-02,\n          -1.2018e-02, -1.4253e-02, -1.8039e-02, -3.3383e-02,  2.4393e-02,\n           6.5606e-03, -1.5476e-02,  2.5994e-02, -6.5150e-03,  3.5169e-02,\n          -1.0532e-02, -1.6271e-02, -2.9575e-02,  2.4816e-02, -2.2262e-02,\n          -3.0730e-02, -3.4432e-02, -3.0334e-02,  3.3999e-02,  2.6709e-02,\n          -1.0255e-03, -3.3446e-02,  2.0564e-02,  4.6238e-03,  2.4569e-02],\n         requires_grad=True)),\n ('0.weight_orig',\n  Parameter containing:\n  tensor([[-0.0075, -0.0188,  0.0311,  ..., -0.0330, -0.0178,  0.0280],\n          [ 0.0076,  0.0107,  0.0211,  ...,  0.0009, -0.0283,  0.0081],\n          [-0.0292, -0.0038,  0.0200,  ..., -0.0116, -0.0153,  0.0259],\n          ...,\n          [ 0.0324,  0.0086,  0.0115,  ...,  0.0066,  0.0319, -0.0295],\n          [-0.0009,  0.0058,  0.0264,  ..., -0.0131, -0.0017, -0.0036],\n          [ 0.0344,  0.0166, -0.0171,  ..., -0.0148, -0.0139, -0.0031]],\n         requires_grad=True)),\n ('2.bias',\n  Parameter containing:\n  tensor([ 0.0156,  0.0045, -0.0490, -0.0184, -0.0470,  0.0100, -0.0542, -0.0533,\n           0.0519,  0.0364,  0.0454, -0.0529,  0.0523,  0.0547, -0.0327, -0.0324,\n           0.0176,  0.0187, -0.0299,  0.0405, -0.0064, -0.0513,  0.0046,  0.0309,\n           0.0423,  0.0432,  0.0423,  0.0197, -0.0101,  0.0221, -0.0200, -0.0506,\n          -0.0377, -0.0532, -0.0197, -0.0356, -0.0261, -0.0503,  0.0496,  0.0364,\n          -0.0544, -0.0517, -0.0472, -0.0179,  0.0139,  0.0524, -0.0050,  0.0400,\n          -0.0111,  0.0032,  0.0389, -0.0341, -0.0483,  0.0407, -0.0137, -0.0455,\n          -0.0481, -0.0374, -0.0344,  0.0266,  0.0526, -0.0123, -0.0342, -0.0421,\n           0.0418, -0.0268,  0.0341,  0.0403,  0.0090, -0.0517,  0.0554, -0.0048,\n          -0.0304, -0.0300, -0.0016,  0.0082,  0.0042,  0.0025, -0.0137,  0.0422,\n           0.0352,  0.0225, -0.0095, -0.0268, -0.0498, -0.0022, -0.0050, -0.0523,\n           0.0367, -0.0502,  0.0510, -0.0079,  0.0178, -0.0544,  0.0325,  0.0331,\n           0.0244, -0.0470,  0.0190,  0.0458], requires_grad=True)),\n ('2.weight_orig',\n  Parameter containing:\n  tensor([[ 0.0139,  0.0486, -0.0121,  ...,  0.0364, -0.0561,  0.0465],\n          [ 0.0476,  0.0530,  0.0148,  ..., -0.0194,  0.0482,  0.0416],\n          [-0.0459,  0.0330,  0.0259,  ..., -0.0174, -0.0471, -0.0201],\n          ...,\n          [-0.0114,  0.0337,  0.0479,  ..., -0.0159,  0.0263,  0.0418],\n          [ 0.0090,  0.0075,  0.0214,  ..., -0.0370,  0.0244, -0.0208],\n          [ 0.0044,  0.0287, -0.0143,  ...,  0.0542,  0.0049, -0.0131]],\n         requires_grad=True)),\n ('4.bias',\n  Parameter containing:\n  tensor([-0.0920,  0.0335,  0.0502, -0.0449, -0.0021, -0.0986, -0.0785,  0.0937,\n          -0.0221, -0.0823], requires_grad=True)),\n ('4.weight_orig',\n  Parameter containing:\n  tensor([[-2.8037e-02, -5.7036e-02, -4.2782e-02, -5.9284e-02,  4.1425e-02,\n           -1.7412e-02,  9.0237e-02, -6.7598e-02,  4.5985e-02,  5.2013e-02,\n            5.8540e-02, -7.2791e-02, -5.4090e-02, -9.8626e-02, -4.2066e-02,\n           -9.4617e-02, -1.8904e-02,  2.4629e-02,  4.3091e-03,  7.3126e-02,\n           -6.5572e-03,  3.8737e-02,  8.9397e-02,  8.8698e-02,  9.1065e-02,\n            8.1114e-02, -8.9895e-02, -7.5269e-02,  9.2847e-02, -5.7281e-02,\n            7.0357e-02,  9.6965e-02,  2.5996e-03, -8.1943e-03,  5.1597e-02,\n           -3.0114e-02, -3.9259e-02,  8.8142e-02,  6.1178e-02,  5.7984e-02,\n            3.0066e-02,  2.1207e-03,  6.1633e-02, -3.8409e-02, -7.8061e-02,\n           -3.6836e-02,  7.6919e-02,  4.0536e-02, -8.2927e-02,  4.4819e-02,\n           -4.9333e-02, -2.3156e-02, -5.2970e-02,  7.4766e-02, -1.6647e-02,\n           -4.5042e-02, -1.6403e-02, -3.9051e-02,  4.6017e-02,  7.7483e-02,\n           -2.6933e-02,  1.3742e-02,  4.7609e-03, -3.5444e-02,  9.6398e-02,\n           -7.7756e-03, -5.3522e-02,  1.0420e-04,  8.3699e-02, -9.6092e-02,\n            8.6788e-02,  5.6665e-02, -4.0672e-02, -8.1857e-02,  3.5794e-02,\n           -4.9819e-02, -9.7172e-02,  8.9797e-02, -5.9009e-02,  6.1692e-02,\n           -6.5248e-02,  6.5269e-02,  6.7018e-02,  3.6698e-02, -7.8142e-02,\n           -7.5511e-02,  7.5971e-03, -2.3215e-02,  8.9926e-02, -7.3029e-02,\n            6.5101e-02, -9.6064e-02,  2.9458e-02,  4.3391e-02,  3.9856e-02,\n            7.7306e-02, -9.9565e-02, -3.6723e-02, -1.9159e-02,  7.1046e-02],\n          [-2.8458e-02,  3.9360e-02, -3.4532e-02,  6.3899e-02, -1.1628e-02,\n           -3.2823e-02, -4.9411e-02,  9.0657e-02,  7.2988e-02,  4.7357e-02,\n           -1.0747e-02, -1.9448e-03, -8.3413e-02, -1.5582e-02,  5.2392e-02,\n            1.3978e-02, -6.0600e-02, -8.6512e-02,  5.2301e-02,  4.3978e-02,\n           -2.0600e-02, -8.1357e-02,  5.0202e-02,  2.0343e-03,  5.2833e-02,\n           -3.9755e-02,  6.3108e-02, -9.1133e-02, -3.8508e-02, -4.9103e-02,\n            8.1061e-02, -3.5234e-02, -8.4986e-02,  8.6141e-02,  4.2691e-02,\n            3.2084e-02, -4.1923e-02,  3.7216e-02,  5.4806e-02, -1.3156e-02,\n            4.5898e-02,  5.3345e-02,  4.5752e-02, -8.8770e-02, -6.4395e-02,\n            7.9649e-02, -6.1341e-02, -7.6353e-02, -5.7102e-03, -1.1760e-03,\n            9.3190e-02,  7.4128e-02, -4.3365e-02, -3.2310e-02,  9.3436e-02,\n            2.3058e-02,  1.2828e-02, -3.7747e-02, -5.8215e-02, -8.7550e-02,\n            1.8158e-03, -1.5700e-02,  5.5653e-02, -9.8088e-02, -3.0799e-02,\n           -3.0614e-02, -5.4910e-02,  8.0142e-02,  4.4758e-03,  2.1109e-02,\n           -6.5028e-02,  3.9044e-03,  9.0425e-02,  7.2258e-02, -5.7219e-02,\n            2.5993e-02, -3.4852e-02, -2.9538e-02, -9.9582e-02,  5.7123e-02,\n            2.8266e-02, -9.0193e-02,  7.6788e-02,  6.3591e-02, -7.4655e-02,\n            5.0564e-02, -5.6549e-02,  7.8619e-05,  2.0556e-02,  4.4474e-02,\n           -1.5472e-02, -1.6201e-02, -7.5972e-03, -5.5273e-02, -3.8846e-02,\n           -1.0902e-02,  9.3163e-02,  1.4173e-02, -2.4927e-02,  9.4968e-02],\n          [ 6.0832e-02,  5.8817e-02, -3.1427e-02,  7.3986e-02,  6.3001e-02,\n           -9.4541e-02,  6.6944e-02,  3.3368e-02,  9.2560e-02,  6.4214e-02,\n           -6.9769e-02, -8.5669e-02, -2.4215e-02,  2.7755e-03, -7.1528e-02,\n            8.3064e-02, -3.6644e-02, -4.7837e-02, -3.6679e-02, -1.8687e-02,\n           -2.3795e-02, -3.3752e-02,  3.9503e-02, -3.5241e-02, -1.7964e-02,\n           -1.1067e-02,  8.8750e-02,  5.4684e-02,  7.1737e-02,  1.1764e-02,\n           -8.3427e-02, -9.9638e-02, -4.7651e-02,  8.6478e-02,  8.4285e-02,\n           -8.8471e-02,  5.3325e-02,  9.4465e-02, -1.8108e-02,  6.8732e-03,\n            1.8204e-02, -3.7696e-02, -8.0270e-02,  9.0182e-02,  8.1443e-02,\n           -9.7336e-02, -6.6514e-02,  7.7320e-02,  1.0445e-02,  5.5933e-02,\n           -8.8162e-02, -1.8670e-02, -2.5312e-02, -3.7258e-02,  9.1375e-02,\n           -8.2767e-02,  9.4633e-02, -1.9016e-02, -6.8361e-02, -4.2003e-02,\n            4.4054e-02,  5.5189e-02,  8.4542e-02,  8.1974e-02, -5.7284e-03,\n            8.7454e-02,  3.7477e-03, -1.2076e-03, -1.3054e-03,  1.2053e-02,\n            9.3970e-02, -5.5257e-02,  7.8011e-02,  1.9402e-02,  9.5751e-02,\n            5.2662e-02,  8.1503e-03, -5.0408e-02, -9.7240e-02,  6.9840e-02,\n           -1.9826e-02, -3.5079e-04, -6.4864e-02, -9.9628e-02, -8.8498e-02,\n           -6.5162e-02, -5.8746e-03, -4.8061e-02,  9.8070e-02, -1.3284e-02,\n            9.2730e-02,  7.0892e-02, -8.1807e-02,  5.5163e-03,  3.9701e-02,\n            5.4651e-02,  2.3001e-02,  4.7995e-02, -6.1961e-02, -9.8708e-02],\n          [ 8.4394e-02,  6.2959e-02,  5.4715e-02,  7.7197e-02, -8.0892e-02,\n           -4.4574e-02,  7.7303e-03, -3.9449e-02,  8.7297e-02,  7.7678e-03,\n           -9.6013e-02, -5.0568e-02,  7.7899e-02,  1.8065e-02, -3.0181e-02,\n           -8.8318e-02,  7.9880e-02, -6.5357e-02, -4.6182e-02, -7.2471e-02,\n            1.0663e-02,  9.6842e-02, -1.8601e-02,  9.2961e-02,  2.9013e-02,\n            3.0830e-02, -1.7062e-02, -2.2864e-02, -6.4398e-02, -9.2438e-02,\n            3.0436e-02, -1.7231e-02, -8.4512e-02, -5.6958e-02,  5.6766e-03,\n           -9.2663e-02, -5.0538e-02,  1.1435e-02, -7.7753e-02,  6.7220e-02,\n            1.7219e-02,  5.7272e-02,  4.4348e-02,  6.7541e-03, -6.1032e-02,\n            9.9242e-02,  6.0087e-02, -6.7087e-02, -6.9499e-02, -4.2058e-02,\n           -9.6338e-02,  8.7058e-02,  3.8440e-02, -5.4056e-02,  6.6410e-02,\n           -1.8061e-02, -3.2400e-03,  9.1628e-02,  5.9732e-02,  2.5843e-03,\n            7.0332e-02,  8.1723e-02, -5.1074e-02, -7.8356e-02,  7.5071e-02,\n           -3.4734e-02,  2.4134e-02,  8.1059e-02, -4.1667e-02, -6.0258e-02,\n           -7.6022e-02,  6.0521e-02, -6.5902e-02, -3.5757e-03,  2.5124e-02,\n            8.9622e-02, -3.9887e-02,  6.7496e-02, -5.2585e-02,  6.9058e-04,\n           -6.3323e-02,  4.4613e-02,  8.5273e-02,  3.9484e-02,  2.9978e-02,\n           -3.4878e-02, -3.7548e-02, -5.2732e-02, -6.6063e-02,  6.8207e-03,\n           -3.2289e-02,  4.9075e-02, -2.8087e-02, -6.1360e-02,  2.5126e-02,\n           -8.3097e-02,  1.8569e-02, -9.5272e-02,  5.9486e-02, -4.5804e-02],\n          [-9.3933e-02,  1.8753e-02,  3.0081e-03,  8.5241e-02,  4.7900e-02,\n            5.3085e-02, -2.1221e-02,  9.4868e-02,  8.8515e-02,  5.9956e-02,\n           -5.9480e-02, -5.1012e-02,  6.4440e-02, -5.1328e-02,  1.5808e-02,\n           -3.8295e-02, -5.0165e-02, -7.6045e-03, -1.5332e-02,  1.1867e-02,\n            5.5693e-02, -1.0953e-02,  2.6927e-02, -4.9655e-03, -3.1339e-02,\n           -9.2112e-02,  1.5439e-02, -6.8958e-02,  3.0602e-03,  9.2165e-02,\n           -8.0014e-02, -2.9030e-04, -4.9686e-02,  4.5785e-02, -8.9618e-02,\n           -2.5667e-02, -2.4437e-02, -8.7262e-02,  4.6825e-02,  3.7820e-02,\n            4.7954e-02,  1.4436e-02, -5.9633e-02,  9.3987e-02, -4.2035e-02,\n           -9.8740e-02, -4.9935e-02,  5.2043e-02, -3.6957e-02,  5.7509e-02,\n            3.4236e-02, -5.5194e-02, -2.5188e-02, -5.0136e-02, -9.2053e-02,\n            8.8468e-02, -7.5905e-02, -3.5588e-02, -4.4326e-02, -6.6500e-02,\n           -3.0951e-02, -6.7242e-02, -9.4255e-02, -3.3857e-02,  2.8445e-02,\n           -5.7698e-02, -9.6360e-02, -8.3355e-03, -2.2718e-02, -2.7477e-02,\n            5.2017e-02, -5.4682e-02,  8.1934e-02,  7.4308e-02, -3.7537e-02,\n            7.1209e-02, -7.1630e-02,  7.5049e-03,  2.7328e-02,  5.2258e-02,\n           -4.2733e-02,  6.9073e-03, -2.9354e-04, -6.5589e-02, -9.4792e-03,\n           -3.2192e-02,  5.9222e-02,  5.2607e-02, -3.8185e-02, -6.8177e-02,\n            8.0894e-02, -1.0016e-02, -6.5890e-02, -2.2263e-03, -5.0598e-02,\n            1.9254e-02,  7.0847e-02,  8.1297e-02,  7.4968e-02, -5.8315e-02],\n          [-6.0530e-02, -1.5787e-02, -3.9852e-02,  1.2282e-02,  2.5778e-03,\n            7.9038e-02, -7.2515e-02,  1.9336e-02, -3.2506e-02, -9.8750e-02,\n           -8.4322e-02, -7.1908e-02, -9.2827e-02, -4.3615e-02, -1.5409e-02,\n            6.4288e-02, -9.1499e-02,  9.1344e-02, -7.8321e-02,  8.6573e-02,\n           -8.0419e-02, -2.2939e-02,  4.8622e-02, -2.3557e-03,  6.9564e-02,\n           -2.7701e-02, -4.6086e-02, -5.3689e-02, -9.1344e-02,  3.1188e-02,\n            5.3154e-02, -9.8527e-03,  6.2865e-02,  6.3547e-03, -7.4157e-02,\n            1.0859e-02, -4.0514e-02,  9.3756e-02,  8.2646e-02,  2.9362e-02,\n           -5.0030e-02, -2.0335e-02,  5.9911e-02,  8.5875e-02,  7.2953e-02,\n           -1.2191e-02,  1.4459e-02, -2.0405e-02,  6.0831e-02, -4.5124e-02,\n            2.1709e-02, -2.0239e-02, -6.7328e-02, -3.5270e-02,  8.5017e-02,\n           -3.1969e-02, -3.0566e-03, -3.1395e-02,  9.0307e-02,  1.9554e-02,\n           -9.4246e-02,  9.8736e-02,  3.1158e-04,  5.1641e-02,  8.9772e-03,\n            5.3041e-03, -8.7644e-02, -7.6724e-02,  1.9144e-03, -9.9216e-02,\n           -1.6374e-02, -2.1402e-02,  6.0523e-02,  2.1308e-02,  3.3579e-02,\n           -4.5133e-02, -3.3078e-02,  2.5111e-02, -4.4171e-03,  5.1837e-03,\n           -4.9010e-02, -2.9845e-02, -3.2288e-02,  4.8682e-02, -9.6229e-02,\n           -4.5997e-02, -6.1152e-02, -6.5661e-02,  9.5437e-03,  2.6270e-02,\n           -4.7047e-02, -9.3340e-02, -7.9909e-03,  6.9246e-02, -1.0306e-02,\n            5.7078e-02,  2.6828e-02,  7.1422e-02,  5.9186e-02,  8.8874e-02],\n          [-8.1643e-02,  2.3888e-02,  7.3460e-02, -7.0057e-02, -6.2378e-02,\n            3.0632e-02, -6.6942e-02, -5.8412e-02, -3.5578e-02,  7.3483e-02,\n            8.1133e-02, -9.2847e-02, -3.6634e-02,  7.4138e-02, -8.5820e-02,\n           -5.2192e-02, -4.9951e-02, -8.5877e-03,  5.9014e-02,  6.6593e-03,\n           -4.1550e-02,  1.1164e-02, -5.4421e-02,  1.4514e-02,  3.9317e-03,\n           -2.5658e-02, -6.6557e-02,  9.6649e-02, -9.7109e-02,  6.0592e-02,\n            3.4955e-02, -2.1478e-02,  3.5619e-02,  3.9552e-02, -5.9462e-02,\n            9.7947e-02, -7.1137e-03, -7.5615e-03, -9.8259e-02, -2.3224e-02,\n           -2.2798e-02,  6.8833e-02, -3.3522e-02, -4.8593e-02,  9.1118e-02,\n            5.2560e-02, -9.9389e-03, -9.5857e-02, -3.3389e-02,  8.9805e-02,\n            5.3339e-02, -3.1387e-02, -4.3199e-02,  1.5231e-02,  6.6662e-02,\n            8.3533e-03,  1.1394e-02,  5.5739e-02,  3.4234e-02,  2.1520e-02,\n            6.1603e-02,  4.0677e-02,  5.1440e-02,  5.3947e-02,  1.0694e-02,\n            2.5190e-02,  2.8379e-02, -6.9102e-03, -5.7999e-02,  2.5940e-02,\n           -9.8905e-02, -9.5909e-02,  7.2515e-02,  7.1270e-02,  9.0440e-02,\n            8.0969e-02,  1.6935e-02, -3.7620e-02,  1.9296e-02, -4.5755e-02,\n            6.8363e-02,  5.0702e-04,  8.3848e-04, -2.7698e-02,  8.3706e-02,\n           -3.1722e-02, -8.8961e-02,  5.4914e-02, -4.0948e-02,  6.3262e-02,\n            4.8308e-02,  6.1001e-02, -4.6835e-02,  9.0753e-02,  2.3657e-02,\n           -3.5311e-02,  7.3217e-02,  9.1227e-02,  7.2262e-02, -1.4488e-02],\n          [ 6.0767e-02, -8.1142e-02,  4.6984e-02, -2.6996e-02,  9.9036e-02,\n           -2.5268e-03, -7.2579e-02,  7.0471e-02,  6.3293e-03,  6.5144e-02,\n            5.8133e-02, -6.9137e-02, -9.7233e-02, -4.1325e-02, -6.6279e-02,\n           -4.0308e-02,  6.7183e-02, -5.8848e-02,  4.6641e-02,  3.3229e-02,\n            7.2117e-02,  9.4853e-02, -3.6129e-02, -2.6159e-02,  3.8959e-02,\n            9.6179e-02, -6.2697e-02,  3.5541e-02, -9.4751e-02, -5.8262e-02,\n           -1.7186e-02, -2.5885e-02, -1.5241e-02,  9.9908e-02, -6.5652e-02,\n            5.9446e-02, -5.5815e-02,  1.4627e-02, -1.1107e-02, -7.0954e-02,\n           -9.0619e-02,  9.5516e-02, -6.8547e-02, -2.8887e-02, -7.2331e-02,\n           -6.9980e-02,  6.9150e-02, -1.4970e-02,  3.5084e-02,  6.4224e-02,\n            6.3009e-02,  1.2033e-02, -4.3873e-02,  9.7219e-02,  8.0897e-02,\n           -3.3933e-02,  5.9763e-03,  2.5897e-02, -4.5720e-02,  9.6570e-02,\n           -7.9146e-02, -8.5930e-02,  4.0905e-02,  2.0830e-02,  1.1960e-04,\n            9.4351e-02,  5.1207e-02, -7.0805e-02,  9.1210e-02, -6.7855e-02,\n            7.1735e-02,  7.6244e-02, -2.0672e-02,  8.1444e-02,  2.3196e-02,\n           -2.5309e-02, -2.2645e-02,  4.0579e-02,  9.4895e-02, -3.7758e-02,\n           -7.4549e-03, -9.1777e-03, -5.6637e-02,  3.8209e-02, -9.2337e-02,\n           -9.0739e-02, -5.7288e-03, -8.3004e-02,  3.4277e-02,  3.1973e-03,\n            9.4356e-02, -4.4922e-02, -7.3153e-02, -9.2990e-02,  9.5319e-02,\n            8.1185e-02, -2.4443e-02,  9.3135e-02, -9.8044e-02, -2.7194e-02],\n          [-4.1821e-02,  4.8412e-02,  4.4647e-02, -6.5829e-02,  7.5461e-02,\n           -6.2174e-02, -3.4066e-02,  9.1181e-02, -3.0311e-02, -8.5923e-02,\n            2.9330e-03, -3.2946e-02, -8.3789e-02,  8.5817e-02,  3.6470e-02,\n           -8.8338e-02,  2.2605e-02,  6.7801e-02, -7.9042e-02,  7.7274e-02,\n           -6.5775e-02, -7.6026e-02,  2.8565e-02,  3.5821e-02, -8.3326e-02,\n            2.8112e-02, -5.7887e-02,  5.5817e-02, -4.7364e-02, -5.7234e-02,\n            7.3678e-02,  1.7393e-03, -6.7671e-03, -1.6896e-02,  1.4930e-02,\n           -1.9202e-02, -1.0311e-02,  2.6490e-02,  5.9821e-02, -1.7477e-02,\n           -2.3757e-02, -6.8921e-02, -9.8486e-02, -5.1972e-02, -5.4510e-02,\n           -3.0212e-02, -9.0008e-02,  9.3999e-02,  6.9808e-02,  5.4767e-02,\n            3.5357e-03,  7.2961e-02,  5.1154e-02,  7.3827e-02,  7.4527e-02,\n           -6.4590e-02,  5.9083e-02, -9.5475e-02, -7.3123e-02, -8.6027e-02,\n            4.3190e-02, -8.8504e-02,  7.0120e-02, -7.4775e-02, -4.5146e-02,\n           -2.0623e-02,  4.0839e-02,  9.2106e-02, -3.3462e-02, -9.2561e-02,\n            6.3755e-02,  4.4677e-03, -4.3969e-02, -4.5708e-02, -2.9535e-02,\n            5.4036e-02, -9.6383e-02,  3.2218e-02, -1.0148e-02, -8.1964e-02,\n           -2.1989e-02,  3.7218e-03,  3.1665e-02, -1.0570e-02,  3.7572e-02,\n           -8.1061e-02,  9.8817e-02,  9.0113e-02,  7.8495e-02, -9.3231e-02,\n           -5.4805e-02, -8.1068e-02, -9.6308e-02, -7.9296e-02,  8.0694e-02,\n            2.3741e-02, -7.6161e-02,  2.8462e-03,  8.6837e-02, -5.6468e-02],\n          [-5.5026e-02,  1.3992e-02,  1.8720e-03,  2.9303e-02, -2.9867e-02,\n            7.5478e-02, -9.2438e-02, -1.9575e-02,  1.8462e-02, -6.8194e-02,\n           -7.7483e-02, -4.8242e-02,  1.7045e-02,  5.9488e-02,  9.3313e-02,\n           -7.1958e-03, -3.2466e-02, -2.0984e-02, -2.8236e-02,  9.7844e-02,\n           -6.3106e-02, -9.5019e-02,  8.6543e-02,  6.0822e-02, -1.1614e-02,\n           -2.4226e-02,  4.0360e-02,  9.2272e-02,  4.4445e-02,  5.1333e-02,\n           -2.5663e-02,  1.8630e-02, -4.7715e-02,  7.7550e-02,  1.1545e-02,\n            4.9691e-03,  6.2113e-02, -6.3308e-02, -5.7970e-02, -1.8114e-02,\n           -1.5882e-02, -4.3451e-03,  1.0514e-03,  3.9963e-02, -2.8748e-02,\n           -5.3282e-02,  1.9250e-02, -4.1497e-02, -3.5370e-02, -5.2510e-02,\n           -8.4755e-02, -5.9825e-03, -8.9925e-02,  4.6789e-02, -5.5625e-02,\n            4.5465e-02, -5.7907e-02,  8.6532e-02, -5.5229e-02, -1.2665e-02,\n           -5.3560e-02,  8.6438e-02, -1.3419e-02, -1.4002e-02, -7.4507e-03,\n           -6.2879e-02, -9.5823e-02, -6.4114e-02, -5.7645e-02, -4.4455e-02,\n           -5.0035e-02, -5.7432e-02, -1.9619e-02, -1.0580e-03, -8.5861e-02,\n            3.6171e-03, -7.5657e-02, -3.4322e-02, -2.6196e-02, -1.9707e-02,\n           -9.7260e-02, -5.6547e-02, -4.8866e-03,  5.4882e-02,  8.7961e-02,\n           -7.5769e-02,  9.7404e-02,  6.9065e-02,  7.1408e-02,  3.1873e-02,\n            6.7208e-02,  4.2981e-03, -5.6753e-02, -7.2886e-02, -4.4058e-02,\n            3.4715e-02,  3.1368e-02,  5.3284e-02,  4.6585e-02,  2.8107e-02]],\n         requires_grad=True))]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(net_out.named_parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:41.057684Z",
     "end_time": "2023-11-02T18:53:41.338326Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Define a function that randomly prunes a network.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Function implemented as: `TODO` in `dl_assignment_7_common.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Check the above function.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net: lenet Before pruning: 266610 After pruning: 213470 Pruned: 19.93\n",
      "Net: resnet18 Before pruning: 282308 After pruning: 226055 Pruned: 19.93\n",
      "Net: vgg19 Before pruning: 20024404 After pruning: 16020628 Pruned: 19.99\n"
     ]
    }
   ],
   "source": [
    "for net_type in [\"lenet\", \"resnet18\", \"vgg19\"]:\n",
    "    net = create_network(net_type)\n",
    "    params_before = sum(p.numel() for p in net.parameters())\n",
    "    prune_network(net, net_type, 0.2, prune_type=prune.RandomUnstructured)\n",
    "    params_after = params_before - sum(torch.sum(b == 0) for b in net.buffers()).item()\n",
    "    print(\"Net: \" + net_type + \" Before pruning: \" + str(params_before) + \" After pruning: \" + str(params_after) +\n",
    "          \" Pruned: \" + str(round(abs(1-(params_after / params_before))*100, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-06T16:46:26.319410Z",
     "end_time": "2023-11-06T16:46:33.991428Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) Define a function that performs the experiment described in Section 1 of the LTH paper on a given dataset and with a given architecture.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "Save all needed results to a file, such as test loss and accuracy. This will make your job easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Function implemented as: `TODO` in `dl_assignment_7_common.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Confirming the Lottery Ticket Hypothesis (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Perform the experiments needed to reproduce the red lines in Figure 1 from the LTH paper.<span style=\"float:right\"> (6 points)</span>**\n",
    "\n",
    "* It is ok to ignore the error bars for now, and focus on doing one series of experiments.\n",
    "* You may also reduce the number of points in the plot to keep the computation time managable.\n",
    "* You do not have to match the visual style of the figure.\n",
    "\n",
    "Hint: create a python script (`experiment1-{dataset}-{method}.py`) that does all the training as needed.\n",
    "Then load the checkpoint files and do your analysis here.\n",
    "You may also want to define more helper functions.\n",
    "\n",
    "Hint 2: look at previous assignments for how to plot. If you do want to include error bars, see also [documentation for `plt.errorbar`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Training implemented in `TODO.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:45.023395Z",
     "end_time": "2023-11-02T18:53:45.296348Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Do your results match the paper? Discuss similarities and differences.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) What can you conclude from this experiment?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Experiments from Section 2 (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) What is the difference between the experiment in Figure 1 and Figure 3 of the paper.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: are there differences in the method, the architecture, or the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Implement the iterative pruning method from the paper.<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Function implemented as: `TODO` in `dl_assignment_7_common.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Perform the experiments needed to reproduce Figure 4a from the paper.<span style=\"float:right\"> (4 points)</span>**\n",
    "\n",
    "Hint: see previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Training implemented in `TODO.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-02T18:53:45.296348Z",
     "end_time": "2023-11-02T18:53:45.540868Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Do your results match the paper? Discuss similarities and differences.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) What can you conclude from this experiment?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Experiments from Section 4 (7 points)\n",
    "\n",
    "Section 3 and 4 deal with convolutional neural networks. We are going to skip the networks in section 3, and move on to Figure 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Section 4 of the paper describes a slightly different pruning method. Implement that method.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "Hint: look at [`torch.nn.utils.prune.global_unstructured`](https://pytorch.org/docs/stable/generated/torch.nn.utils.prune.global_unstructured.html) and at the examples on that page.\n",
    "\n",
    "If you get stuck on this step, you can continue with the same pruning methods as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Function implemented as: `TODO` in `dl_assignment_7_common.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Implement a function that constructs the network architecture used in Figure 8.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Extend the `create_network` function defined earlier.\n",
    "\n",
    "Hint: VGG16 and Resnet18 are [predefined in torchvision](https://pytorch.org/vision/0.12/models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Perform the experiments needed to reproduce Figure 8 from the LTH paper.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "* Again: you do not need to include error bars.\n",
    "* You may limit yourself to one of the figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Training implemented in `TODO.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Do your results match the paper? Discuss similarities and differences.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 65 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 97b3d19 / 2023-10-19</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
